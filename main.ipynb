{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os, logging\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position=torch.arange(max_len).unsqueeze(1)\n",
    "        div_term=1/(10000**(torch.arange(0,d_model,2)/d_model))\n",
    "        pe=torch.zeros(max_len,d_model)\n",
    "        pe[:,0::2]=torch.sin(position*div_term)\n",
    "        pe[:,1::2]=torch.cos(position*div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self,patches):\n",
    "        return self.dropout(patches+self.pe[:patches.shape[1]])\n",
    "\n",
    "\n",
    "\n",
    "# class ImgPatcher(nn.Module):\n",
    "#     def __init__(self,n_patches:int,emb_d:int,channels_first:bool,img_shape:tuple):\n",
    "#         super(ImgPatcher,self).__init__()\n",
    "#         self.channels_first=channels_first\n",
    "#         H,W,C=img_shape\n",
    "#         if self.channels_first:\n",
    "#             C,H,W=img_shape\n",
    "#         self.chw=(C,H,W)\n",
    "#         assert H==W, \"Please make sure image has same height and width\"\n",
    "#         assert H%n_patches==0, \"Please make sure n_patches is factor of image dims H and W\"\n",
    "#         self.n_patches=n_patches\n",
    "#         self.p=H//n_patches\n",
    "#         self.embedding=nn.Linear(H*W*C//n_patches**2,emb_d)                            #n_patches^2 patches of size (H/n_patches*W/n_patches)*C will be created from each image\n",
    "#         print(self.embedding.weight.shape)\n",
    "    \n",
    "#     def forward(self,img):\n",
    "#         img=img.float()\n",
    "#         N=img.size(0)\n",
    "#         if self.channels_first:\n",
    "#             img=img.permute(0,2,3,1)                #N,C,H,W -> N,H,W,C\n",
    "#         patches=torch.zeros(N,self.n_patches**2,self.embedding.weight.shape[1])\n",
    "#         for idx in range(N):\n",
    "#             img_=img[idx]\n",
    "#             for i in range(self.n_patches):\n",
    "#                 for j in range(self.n_patches):\n",
    "#                     patch=img_[i*self.p:(i+1)*self.p,j*self.p:(j+1)*self.p]\n",
    "#                     patches[idx,(i*self.n_patches)+j]=patch.flatten()\n",
    "#         return self.posenc(self.embedding(patches))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.io\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def load_tokenizer(tokenizer_model:str,data_dir:str=None):\n",
    "    if not os.path.exists('data/tokenizer'):\n",
    "        tokenizer=torch.hub.load('huggingface/pytorch-transformers', 'tokenizer',tokenizer_model)\n",
    "        logging.info(\"Updating Tokenizer Vocab on Data\")\n",
    "        #insert code to get to process data_dir\n",
    "        #[tokenizer(i) for i in tqdm(data_dir[:,1])]\n",
    "        tokenizer.save_pretrained('data/tokenizer')\n",
    "        return tokenizer\n",
    "    else:\n",
    "        tokenizer=torch.hub.load('huggingface/pytorch-transformers', 'tokenizer','data/tokenizer/')\n",
    "        return tokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Stream_Dataset(Dataset):\n",
    "    def __init__(self,seq_length,data_dir,img_dir,tokenizer_model='medicalai/ClinicalBERT'):\n",
    "        self.data_dir=data_dir\n",
    "        self.data=pd.read_csv(data_dir+'/'+'result0.csv').to_numpy()\n",
    "        self.img_dir=img_dir\n",
    "        self.max_length=seq_length\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    def __getitem__(self,idx):\n",
    "        img_name=self.data[idx,0]\n",
    "        semantic_str=self.data[idx,1]\n",
    "        img=torchvision.io.read_image(self.data_dir+'/'+self.img_dir+'/'+img_name)\n",
    "        return img,semantic_str\n",
    "# dataset=Stream_Dataset(\n",
    "#         seq_length=50,\n",
    "#         data_dir=\"/home/mehedi/Desktop/raghib/FLICKR-30K IMAGE CAPTIONING/flickr30k_images\",\n",
    "#         img_dir='flickr30k_images',\n",
    "#         tokenizer_model='bert-base-uncased'\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from typing import Optional, Any, Union, Callable\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import MultiheadAttention, LayerNorm,Dropout, Linear,TransformerEncoderLayer\n",
    "def _get_activation_fn(activation: str) -> Callable[[Tensor], Tensor]:\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "\n",
    "    raise RuntimeError(\"activation should be relu/gelu, not {}\".format(activation))\n",
    "\n",
    "class CustomTransformerDecoderLayer(nn.Module):\n",
    "    __constants__ = ['batch_first', 'norm_first']\n",
    "\n",
    "    def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
    "                 activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,\n",
    "                 layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False, \n",
    "                 need_attn: bool = True, device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
    "                                            **factory_kwargs)\n",
    "        self.multihead_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
    "                                                 **factory_kwargs)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "\n",
    "        self.norm_first = norm_first\n",
    "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm3 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "        self.dropout3 = Dropout(dropout)\n",
    "        # Legacy string support for activation function.\n",
    "        if isinstance(activation, str):\n",
    "            self.activation = _get_activation_fn(activation)\n",
    "        else:\n",
    "            self.activation = activation\n",
    "        self.need_attn=need_attn\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super().__setstate__(state)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt: Tensor,\n",
    "        memory: Tensor,\n",
    "        tgt_mask: Optional[Tensor] = None,\n",
    "        memory_mask: Optional[Tensor] = None,\n",
    "        tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "        memory_key_padding_mask: Optional[Tensor] = None,\n",
    "        tgt_is_causal: bool = False,\n",
    "        memory_is_causal: bool = False,\n",
    "    ) -> Tensor:\n",
    "        r\"\"\"Pass the inputs (and mask) through the decoder layer.\n",
    "\n",
    "        Args:\n",
    "            tgt: the sequence to the decoder layer (required).\n",
    "            memory: the sequence from the last layer of the encoder (required).\n",
    "            tgt_mask: the mask for the tgt sequence (optional).\n",
    "            memory_mask: the mask for the memory sequence (optional).\n",
    "            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).\n",
    "            memory_key_padding_mask: the mask for the memory keys per batch (optional).\n",
    "            tgt_is_causal: If specified, applies a causal mask as tgt mask.\n",
    "                Mutually exclusive with providing tgt_mask. Default: ``False``.\n",
    "            memory_is_causal: If specified, applies a causal mask as tgt mask.\n",
    "                Mutually exclusive with providing memory_mask. Default: ``False``.\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        # see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf\n",
    "\n",
    "        x = tgt\n",
    "        if self.norm_first:\n",
    "            x = x + self._sa_block(x=self.norm1(x), attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask, is_causal=tgt_is_causal)\n",
    "            x = x + self._mha_block(self.norm2(x), memory, memory_mask, memory_key_padding_mask, memory_is_causal)\n",
    "            x = x + self._ff_block(self.norm3(x))\n",
    "        else:\n",
    "            x_sa = self.norm1(x + self._sa_block(x=x, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask, is_causal=tgt_is_causal))\n",
    "            x,attn=self._mha_block(x, memory, memory_mask, memory_key_padding_mask, memory_is_causal,)\n",
    "            x = self.norm2(x_sa + x)\n",
    "            x = self.norm3(x + self._ff_block(x))\n",
    "\n",
    "        return x,attn\n",
    "\n",
    "\n",
    "    # self-attention block\n",
    "    def _sa_block(self, x: Tensor,\n",
    "                  attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: bool = False) -> Tensor:\n",
    "        x = self.self_attn(x, x, x,\n",
    "                           attn_mask=attn_mask,\n",
    "                           key_padding_mask=key_padding_mask,\n",
    "                           is_causal=is_causal,\n",
    "                           need_weights=False)[0]\n",
    "        return self.dropout1(x)\n",
    "\n",
    "    # multihead attention block\n",
    "    def _mha_block(self, x: Tensor, mem: Tensor,\n",
    "                   attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: bool = False) -> Tensor:\n",
    "        x,attn= self.multihead_attn(x, mem, mem,\n",
    "                                attn_mask=attn_mask,\n",
    "                                key_padding_mask=key_padding_mask,\n",
    "                                is_causal=is_causal,\n",
    "                                average_attn_weights=self.need_attn)\n",
    "\n",
    "        return self.dropout2(x),attn\n",
    "\n",
    "    # feed forward block\n",
    "    def _ff_block(self, x: Tensor) -> Tensor:\n",
    "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        return self.dropout3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer_file_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[386], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mod_pth\u001b[39m=\u001b[39mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m,tokenizer_file_name\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer_file_name' is not defined"
     ]
    }
   ],
   "source": [
    "mod_pth=os.path.join(\"data\",tokenizer_file_name.split('/')[-1],\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mehedi/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "Some weights of the model checkpoint at medicalai/ClinicalBERT were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "119547"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_bert_embedder(tokenizer_model:str,tokenizer=None):\n",
    "    tokenizer_file_name= tokenizer.name if tokenizer else tokenizer_model\n",
    "    assert tokenizer_file_name==tokenizer_model, \"Please make sure the HuggingFace tokenizer and embedder are from same repo\"\n",
    "    mod_pth=os.path.join(\"data\",tokenizer_file_name.split('/')[-1],\"model\")\n",
    "    model=torch.hub.load('huggingface/pytorch-transformers', 'model',(mod_pth if os.path.exists(mod_pth) else tokenizer_model))\n",
    "    # model=torch.hub.load('huggingface/pytorch-transformers', 'model',mod_pth) \n",
    "    model=update_bert_model(model,tokenizer)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def update_bert_model(model,tokenizer=None):\n",
    "    logging.info(\"Updating Model based on Tokenizer\")\n",
    "    tokenizer_file_name=model.name_or_path\n",
    "    tok_pth=os.path.join(\"data\",tokenizer_file_name.split('/')[-1],\"tokenizer\")\n",
    "    if not tokenizer:\n",
    "        print('NOT TOKENIZER')\n",
    "        if not os.path.exists(tok_pth):\n",
    "            raise ValueError(f\"Please update tokenizer with desired dataset and keep at {tok_pth}\")\n",
    "        tokenizer=torch.hub.load('huggingface/pytorch-transformers', 'tokenizer',tok_pth)\n",
    "    model_vocab_len=model.embeddings.word_embeddings.weight.shape[0]\n",
    "    if model_vocab_len!=len(tokenizer):\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        model.save_pretrained(tok_pth.replace('tokenizer','model'))\n",
    "    return model\n",
    "\n",
    "\n",
    "load_bert_embedder(tokenizer_model='bert-base-uncased',tokenizer=tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple,Callable,List,Union\n",
    "from torch import nn\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_bert_embedder(tokenizer_model:str,tokenizer=None):\n",
    "    tokenizer_file_name= tokenizer.name if tokenizer else tokenizer_model\n",
    "    assert tokenizer_file_name==tokenizer_model, \"Please make sure the HuggingFace tokenizer and embedder are from same repo\"\n",
    "    mod_pth=os.path.join(\"data\",tokenizer_file_name.split('/')[-1],\"model\")\n",
    "    model=torch.hub.load('huggingface/pytorch-transformers', 'model',(mod_pth if os.path.exists(mod_pth) else tokenizer_model))\n",
    "    # model=torch.hub.load('huggingface/pytorch-transformers', 'model',mod_pth) \n",
    "    model=update_bert_model(model,tokenizer,tokenizer_model)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def update_bert_model(model,tokenizer=None,tokenizer_model=None):\n",
    "    logging.info(\"Updating Model based on Tokenizer\")\n",
    "    tokenizer_file_name=model.name_or_path if not tokenizer_model else tokenizer_model\n",
    "    tok_pth=os.path.join(\"data\",tokenizer_file_name.split('/')[-1],\"tokenizer\")\n",
    "    if not tokenizer:\n",
    "        if not os.path.exists(tok_pth):\n",
    "            raise ValueError(f\"Please update tokenizer with desired dataset and keep at {tok_pth}\")\n",
    "    model_vocab_len=model.embeddings.word_embeddings.weight.shape[0]\n",
    "    if model_vocab_len<len(tokenizer):\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    model.save_pretrained(tok_pth.replace('tokenizer','model'))\n",
    "    return model\n",
    "\n",
    "\n",
    "class ImgPatcher(nn.Module):\n",
    "    def __init__(self,n_patches:int,patch_dims:int,img_chw:Tuple[int,int,int]):\n",
    "        super(ImgPatcher,self).__init__()\n",
    "        C,H,W=img_chw\n",
    "        assert torch.math.ceil(torch.math.sqrt(n_patches))-torch.math.sqrt(n_patches)==0, \"Please make sure n_patches is of square dimensions\"\n",
    "        assert H==W, \"Please make sure that the image is a square image\"\n",
    "        self.n_patches=n_patches\n",
    "        self.patch_dims=patch_dims\n",
    "        self.kernel_size=int(H//torch.math.sqrt(n_patches))\n",
    "        self.conv=nn.Conv2d(in_channels=C,out_channels=self.patch_dims,kernel_size=self.kernel_size,stride=self.kernel_size)\n",
    "    def forward(self,x):\n",
    "        x=self.conv(x)\n",
    "        x=x.permute(0,2,3,1)\n",
    "        x_emb=x.reshape(x.size(0),self.n_patches,x.size(-1))\n",
    "        return x_emb\n",
    "\n",
    "\n",
    "class SpatialStream(nn.Module):\n",
    "    def __init__(self, n_patches:int,img_chw:Tuple[int,int,int],dropout:float=0.1,d_model:int=768,nhead:int=8,dim_feedforward:int=2048,activation:str='relu',num_ts_blocks:int=2):\n",
    "        super(SpatialStream,self).__init__()\n",
    "        self.patcher=ImgPatcher(n_patches,d_model,img_chw)\n",
    "        self.pos_enc=PositionalEncoding(d_model,dropout,n_patches)\n",
    "        self.spattransformer_blocks=nn.ModuleList([TransformerEncoderLayer(d_model,nhead,dim_feedforward,batch_first=True,activation=activation) for i in range(num_ts_blocks)])\n",
    "        \n",
    "    def forward(self,x):\n",
    "        embedded=self.patcher(x)\n",
    "        encoded=self.pos_enc(embedded)\n",
    "        enc_memory_list=[]\n",
    "        for encoder_layer in self.spattransformer_blocks:\n",
    "            encoded=encoder_layer(encoded)\n",
    "            enc_memory_list.append(encoded)\n",
    "        return enc_memory_list\n",
    "\n",
    "class SemanticStream(nn.Module):\n",
    "    def __init__(self,max_seql:int,tokenizer=None,dropout:float=0.1,num_ts_blocks:int=2,d_model:int=512,nhead:int=8,dim_feedforward:int=2048,activation:str='relu',tokenizer_model='medicalai/ClinicalBERT'):\n",
    "        super(SemanticStream,self).__init__()\n",
    "        self.max_seql=max_seql\n",
    "        self.bert_embedding = load_bert_embedder(tokenizer_model=tokenizer_model,tokenizer=tokenizer)\n",
    "        self.pos_enc=PositionalEncoding(d_model,dropout,max_seql)\n",
    "        self.semtransformer_blocks=nn.ModuleList([CustomTransformerDecoderLayer(d_model,nhead,dim_feedforward,batch_first=True,activation=activation) for _ in range(num_ts_blocks)])\n",
    "        self.out=nn.Linear(d_model,self.bert_embedding.embeddings.word_embeddings.weight.shape[0])\n",
    "    def forward(self,target=None,enc_memory_list=None):\n",
    "        if isinstance(target[0],str):\n",
    "            tgt=tokenizer(target,max_length=self.max_seql,padding='max_length' if self.training else True,truncation=True,return_tensors='pt',add_special_tokens=self.training)\n",
    "            tgt_padmask=tgt['attention_mask']==0\n",
    "            tgt=tgt['input_ids']\n",
    "        else:\n",
    "            tgt=target\n",
    "            tgt_padmask=(tgt==0)\n",
    "        tgt_mask=generate_square_subsequent_mask(tgt.shape[-1])\n",
    "        embedded=self.bert_embedding(input_ids=tgt,attention_mask=tgt_padmask).last_hidden_state\n",
    "        encoded=self.pos_enc(embedded)\n",
    "        for idx,(decoder_layer,memory) in enumerate(zip(self.semtransformer_blocks,enc_memory_list)):\n",
    "            encoded,attn=decoder_layer(encoded,memory,tgt_mask=tgt_mask,tgt_key_padding_mask=tgt_padmask,tgt_is_causal=False)\n",
    "        out=self.out(encoded)\n",
    "        return out,attn\n",
    " \n",
    "\n",
    "    def evaluate(self,enc_memory_list=None):\n",
    "        batch_size=enc_memory_list[0].shape[0]\n",
    "        logits=torch.tensor([tokenizer.vocab['[CLS]'] for _ in range(batch_size)]).reshape(-1,1)\n",
    "        finished_probs=torch.zeros(batch_size,self.max_seql)    \n",
    "        seq_idx=0\n",
    "        completed_idx=[]\n",
    "        incompleted_idx=np.arange(batch_size)\n",
    "\n",
    "        while True:\n",
    "            if logits.shape[1]==self.max_seql or incompleted_idx.size==0:\n",
    "                break\n",
    "            out,_=self.forward(target=logits,enc_memory_list=enc_memory_list)\n",
    "            lastout=out[:,-1].unsqueeze(1)\n",
    "            probs=F.log_softmax(lastout,dim=-1)\n",
    "            top_prob=probs.topk(1)[1].squeeze(-1)\n",
    "            logits=torch.cat([logits,top_prob],axis=1) if seq_idx>0 else top_prob\n",
    "            completed_bool=(top_prob==tokenizer.vocab['[SEP]']).flatten()\n",
    "            c_idx=incompleted_idx[np.nonzero(completed_bool)]\n",
    "            c_idx=c_idx.tolist() if hasattr(c_idx,'__iter__') else [c_idx]\n",
    "            \n",
    "            if len(c_idx)>0:\n",
    "                incompleted_idx=np.array(list(set(incompleted_idx).difference(c_idx)))\n",
    "                completed_idx.extend(c_idx)\n",
    "                finished_probs[c_idx,:seq_idx+1]+=logits[np.where(completed_bool)[0],:seq_idx+1]\n",
    "            logits=logits[completed_bool==False]\n",
    "            seq_idx+=1\n",
    "        if incompleted_idx.size!=0:\n",
    "            finished_probs[incompleted_idx]=logits\n",
    "        out,attn=self.forward(target=finished_probs.int(),enc_memory_list=enc_memory_list)\n",
    "        return out,attn\n",
    "            \n",
    "                \n",
    "\n",
    "    def save_bert(self):\n",
    "        self.bert_embedding.save_pretrained(f'data/{self.bert_embedding.name_or_path.split(\"/\")[-1]}/model/')        \n",
    "        \n",
    "\n",
    "\n",
    "class TwoStreamTransformer(nn.Module):\n",
    "    def __init__(self, n_patches:int,img_chw:Tuple[int,int,int],max_seql:int,dropout:float=0.1,d_model:int=768,nhead:int=8,dim_feedforward:int=2048,activation:str='relu',num_ts_blocks:int=2,tokenizer_model:any='medicalai/ClinicalBERT'):\n",
    "        super(TwoStreamTransformer,self).__init__()\n",
    "        self.encoder=SpatialStream(n_patches,img_chw,dropout,d_model,nhead,dim_feedforward,activation,num_ts_blocks)\n",
    "        self.decoder=SemanticStream(max_seql,dropout,num_ts_blocks,d_model,nhead,dim_feedforward,activation,tokenizer_model)\n",
    "\n",
    "    def forward(self,images,target=None):\n",
    "        enc_memory_list=self.encoder(images)\n",
    "        if not self.training:\n",
    "            logits,attn=self.decoder.evaluate(enc_memory_list=enc_memory_list)\n",
    "        else:\n",
    "            logits,attn=self.decoder(enc_memory_list=enc_memory_list,target=target)\n",
    "        return logits,attn            \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DistilBertModel' object has no attribute 'word_embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[408], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mdecoder\u001b[39m.\u001b[39;49mbert_embedding\u001b[39m.\u001b[39;49mword_embeddings\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/hf_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DistilBertModel' object has no attribute 'word_embeddings'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFTokenizer:\n",
    "    def __init__(self,tokenizer_model,max_length):\n",
    "        tok_pth=os.path.join(\"data\",tokenizer_model.split('/')[-1],\"tokenizer\")\n",
    "        self.name=tokenizer_model\n",
    "        self.tokenizer=torch.hub.load('huggingface/pytorch-transformers', 'tokenizer',tokenizer_model) if not os.path.exists(tok_pth) else torch.hub.load('huggingface/pytorch-transformers', 'tokenizer',tok_pth)\n",
    "        self.save_pth=tok_pth\n",
    "        self.tokenizer.model_max_length=max_length\n",
    "        self.vocab=self.tokenizer.vocab\n",
    "    def __call__(self,data,padding=None,truncation=True,return_tensors='pt',add_special_tokens=True):\n",
    "        return self.tokenizer(data,padding=padding,truncation=truncation,return_tensors=return_tensors,add_special_tokens=add_special_tokens).input_ids\n",
    "    \n",
    "    def decode(self,tok_ids):\n",
    "        return self.tokenizer.decode(tok_ids,skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokenizer)\n",
    "    \n",
    "    def save(self):\n",
    "        self.tokenizer.save_pretrained(self.save_pth)\n",
    "    \n",
    "    def update(self,data:torch.utils.data.DataLoader):\n",
    "        dpoints=0\n",
    "        if isinstance(data,torch.utils.data.DataLoader):\n",
    "            for (img,text) in data:\n",
    "                self.tokenizer(text)\n",
    "                dpoints+=len(text)\n",
    "        elif isinstance(data,Union[np.array,torch.tensor]):\n",
    "            data=data.tolist()\n",
    "            self.tokenizer(data)\n",
    "            dpoints+=len(data)\n",
    "        else:\n",
    "            self.tokenizer(data)\n",
    "            dpoints+=len(data)\n",
    "        print(f\"Updated tokens from {dpoints} rows of data\")\n",
    "        self.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mehedi/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "Some weights of the model checkpoint at medicalai/ClinicalBERT were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'medicalai/ClinicalBERT'"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder=torch.hub.load('huggingface/pytorch-transformers', 'model','medicalai/ClinicalBERT')\n",
    "embedder.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# tokenizer=HFTokenizer('medicalai/ClinicalBERT',20)\n",
    "s=\"I love chicken so much, I want to eat it every day, its\"\n",
    "data=[s[:i] for i in range(0,len(s),10)]\n",
    "decinp=tokenizer(data,padding='max_length',truncation=True,return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "tok,mod=None,None\n",
    "\n",
    "print(not (tok and mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 20])"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decinp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mehedi/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Please update tokenizer with desired dataset and keep at data/model/tokenizer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[420], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m decoder\u001b[39m=\u001b[39mSemanticStream(max_seql\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,num_ts_blocks\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[418], line 73\u001b[0m, in \u001b[0;36mSemanticStream.__init__\u001b[0;34m(self, max_seql, tokenizer, dropout, num_ts_blocks, d_model, nhead, dim_feedforward, activation, tokenizer_model)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39msuper\u001b[39m(SemanticStream,\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m     72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_seql\u001b[39m=\u001b[39mmax_seql\n\u001b[0;32m---> 73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert_embedding \u001b[39m=\u001b[39m load_bert_embedder(tokenizer_model\u001b[39m=\u001b[39;49mtokenizer_model,tokenizer\u001b[39m=\u001b[39;49mtokenizer)\n\u001b[1;32m     74\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_enc\u001b[39m=\u001b[39mPositionalEncoding(d_model,dropout,max_seql)\n\u001b[1;32m     75\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msemtransformer_blocks\u001b[39m=\u001b[39mnn\u001b[39m.\u001b[39mModuleList([CustomTransformerDecoderLayer(d_model,nhead,dim_feedforward,batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,activation\u001b[39m=\u001b[39mactivation) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_ts_blocks)])\n",
      "Cell \u001b[0;32mIn[418], line 17\u001b[0m, in \u001b[0;36mload_bert_embedder\u001b[0;34m(tokenizer_model, tokenizer)\u001b[0m\n\u001b[1;32m     15\u001b[0m model\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mhub\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mhuggingface/pytorch-transformers\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m,(mod_pth \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(mod_pth) \u001b[39melse\u001b[39;00m tokenizer_model))\n\u001b[1;32m     16\u001b[0m \u001b[39m# model=torch.hub.load('huggingface/pytorch-transformers', 'model',mod_pth) \u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m model\u001b[39m=\u001b[39mupdate_bert_model(model,tokenizer)\n\u001b[1;32m     18\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "Cell \u001b[0;32mIn[418], line 28\u001b[0m, in \u001b[0;36mupdate_bert_model\u001b[0;34m(model, tokenizer)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tokenizer:\n\u001b[1;32m     27\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(tok_pth):\n\u001b[0;32m---> 28\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPlease update tokenizer with desired dataset and keep at \u001b[39m\u001b[39m{\u001b[39;00mtok_pth\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m model_vocab_len\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39membeddings\u001b[39m.\u001b[39mword_embeddings\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     30\u001b[0m \u001b[39mif\u001b[39;00m model_vocab_len\u001b[39m<\u001b[39m\u001b[39mlen\u001b[39m(tokenizer):\n",
      "\u001b[0;31mValueError\u001b[0m: Please update tokenizer with desired dataset and keep at data/model/tokenizer"
     ]
    }
   ],
   "source": [
    "decoder=SemanticStream(max_seql=20,num_ts_blocks=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs_0: torch.Size([6, 1, 119547])\n",
      "top_probs_0: torch.Size([6, 1])\n",
      "out_main after completed reeduce: torch.Size([6, 1, 119547])\n",
      "logits_shape: torch.Size([6, 1])\n",
      "torch.Size([6, 1, 119547])\n",
      "####1:  completed_idx: []\n",
      "probs_1: torch.Size([6, 1, 119547])\n",
      "top_probs_1: torch.Size([6, 1])\n",
      "out_main after completed reeduce: torch.Size([5, 2, 119547])\n",
      "logits_shape: torch.Size([5, 2])\n",
      "torch.Size([6, 2, 119547])\n",
      "####2:  completed_idx: [0]\n",
      "probs_2: torch.Size([5, 1, 119547])\n",
      "top_probs_2: torch.Size([5, 1])\n",
      "out_main after completed reeduce: torch.Size([5, 3, 119547])\n",
      "logits_shape: torch.Size([5, 3])\n",
      "torch.Size([5, 3, 119547])\n",
      "####3:  completed_idx: [0]\n",
      "probs_3: torch.Size([5, 1, 119547])\n",
      "top_probs_3: torch.Size([5, 1])\n",
      "out_main after completed reeduce: torch.Size([5, 4, 119547])\n",
      "logits_shape: torch.Size([5, 4])\n",
      "torch.Size([5, 4, 119547])\n",
      "####4:  completed_idx: [0]\n",
      "probs_4: torch.Size([5, 1, 119547])\n",
      "top_probs_4: torch.Size([5, 1])\n",
      "out_main after completed reeduce: torch.Size([4, 5, 119547])\n",
      "logits_shape: torch.Size([4, 5])\n",
      "torch.Size([5, 5, 119547])\n",
      "####5:  completed_idx: [0, 1]\n",
      "probs_5: torch.Size([4, 1, 119547])\n",
      "top_probs_5: torch.Size([4, 1])\n",
      "out_main after completed reeduce: torch.Size([4, 6, 119547])\n",
      "logits_shape: torch.Size([4, 6])\n",
      "torch.Size([4, 6, 119547])\n",
      "####6:  completed_idx: [0, 1]\n",
      "probs_6: torch.Size([4, 1, 119547])\n",
      "top_probs_6: torch.Size([4, 1])\n",
      "out_main after completed reeduce: torch.Size([4, 7, 119547])\n",
      "logits_shape: torch.Size([4, 7])\n",
      "torch.Size([4, 7, 119547])\n",
      "####7:  completed_idx: [0, 1]\n",
      "probs_7: torch.Size([4, 1, 119547])\n",
      "top_probs_7: torch.Size([4, 1])\n",
      "out_main after completed reeduce: torch.Size([3, 8, 119547])\n",
      "logits_shape: torch.Size([3, 8])\n",
      "torch.Size([4, 8, 119547])\n",
      "####8:  completed_idx: [0, 1, 2]\n",
      "probs_8: torch.Size([3, 1, 119547])\n",
      "top_probs_8: torch.Size([3, 1])\n",
      "out_main after completed reeduce: torch.Size([3, 9, 119547])\n",
      "logits_shape: torch.Size([3, 9])\n",
      "torch.Size([3, 9, 119547])\n",
      "####9:  completed_idx: [0, 1, 2]\n",
      "probs_9: torch.Size([3, 1, 119547])\n",
      "top_probs_9: torch.Size([3, 1])\n",
      "out_main after completed reeduce: torch.Size([3, 10, 119547])\n",
      "logits_shape: torch.Size([3, 10])\n",
      "torch.Size([3, 10, 119547])\n",
      "####10:  completed_idx: [0, 1, 2]\n",
      "probs_10: torch.Size([3, 1, 119547])\n",
      "top_probs_10: torch.Size([3, 1])\n",
      "out_main after completed reeduce: torch.Size([2, 11, 119547])\n",
      "logits_shape: torch.Size([2, 11])\n",
      "torch.Size([3, 11, 119547])\n",
      "####11:  completed_idx: [0, 1, 2, 3]\n",
      "probs_11: torch.Size([2, 1, 119547])\n",
      "top_probs_11: torch.Size([2, 1])\n",
      "out_main after completed reeduce: torch.Size([2, 12, 119547])\n",
      "logits_shape: torch.Size([2, 12])\n",
      "torch.Size([2, 12, 119547])\n",
      "####12:  completed_idx: [0, 1, 2, 3]\n",
      "probs_12: torch.Size([2, 1, 119547])\n",
      "top_probs_12: torch.Size([2, 1])\n",
      "out_main after completed reeduce: torch.Size([2, 13, 119547])\n",
      "logits_shape: torch.Size([2, 13])\n",
      "torch.Size([2, 13, 119547])\n",
      "####13:  completed_idx: [0, 1, 2, 3]\n",
      "probs_13: torch.Size([2, 1, 119547])\n",
      "top_probs_13: torch.Size([2, 1])\n",
      "out_main after completed reeduce: torch.Size([1, 14, 119547])\n",
      "logits_shape: torch.Size([1, 14])\n",
      "torch.Size([2, 14, 119547])\n",
      "####14:  completed_idx: [0, 1, 2, 3, 4]\n",
      "probs_14: torch.Size([1, 1, 119547])\n",
      "top_probs_14: torch.Size([1, 1])\n",
      "out_main after completed reeduce: torch.Size([1, 15, 119547])\n",
      "logits_shape: torch.Size([1, 15])\n",
      "torch.Size([1, 15, 119547])\n",
      "####15:  completed_idx: [0, 1, 2, 3, 4]\n",
      "probs_15: torch.Size([1, 1, 119547])\n",
      "top_probs_15: torch.Size([1, 1])\n",
      "out_main after completed reeduce: torch.Size([0, 16, 119547])\n",
      "logits_shape: torch.Size([0, 16])\n",
      "torch.Size([1, 16, 119547])\n",
      "####16:  completed_idx: [0, 1, 2, 3, 4, 5]\n",
      "tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "def forward(target,incompleted_idx,seq_idx):\n",
    "    return torch.nn.functional.one_hot(target[incompleted_idx,:seq_idx+1],num_classes=len(tokenizer))    \n",
    "\n",
    "\n",
    "max_seql=20\n",
    "target=tokenizer(data,truncation=True,padding='max_length')\n",
    "\n",
    "batch_size=target.shape[0]\n",
    "\n",
    "logits=torch.tensor([tokenizer.vocab['[CLS]'] for _ in range(batch_size)]).reshape(-1,1)\n",
    "finished_probs=torch.zeros_like(target,dtype=torch.long)\n",
    "\n",
    "\n",
    "out_main=torch.zeros((batch_size,1,len(tokenizer))).float()+fill_pad.unsqueeze(0)\n",
    "finished_logits=torch.zeros((batch_size,max_seql,len(tokenizer)))\n",
    "\n",
    "seq_idx=0\n",
    "completed_idx=[]\n",
    "incompleted_idx=np.arange(batch_size)\n",
    "\n",
    "while True:\n",
    "    if logits.shape[1]==max_seql or incompleted_idx.size==0:\n",
    "        break\n",
    "    out=forward(target=target,incompleted_idx=incompleted_idx,seq_idx=seq_idx).float()\n",
    "\n",
    "    lastout=out[:,-1].unsqueeze(1)\n",
    "\n",
    "    probs=F.log_softmax(lastout,dim=-1)\n",
    "    print(f\"probs_{seq_idx}: {probs.shape}\")\n",
    "\n",
    "    out_main=torch.cat([out_main,lastout],axis=1) if seq_idx>0 else lastout\n",
    "\n",
    "    top_prob=probs.topk(1)[1].squeeze(-1)\n",
    "    print(f\"top_probs_{seq_idx}: {top_prob.shape}\")\n",
    "    logits=torch.cat([logits,top_prob],axis=1) if seq_idx>0 else top_prob\n",
    "\n",
    "    completed_bool=(top_prob==tokenizer.vocab['[SEP]']).flatten()\n",
    "\n",
    "    c_idx=incompleted_idx[np.nonzero(completed_bool)]\n",
    "    \n",
    "    c_idx=c_idx.tolist() if hasattr(c_idx,'__iter__') else [c_idx]\n",
    "\n",
    "    \n",
    "\n",
    "    if len(c_idx)>0:\n",
    "        incompleted_idx=np.array(list(set(incompleted_idx).difference(c_idx)))\n",
    "        completed_idx.extend(c_idx)\n",
    "\n",
    "        finished_probs[c_idx,:seq_idx+1]+=logits[np.where(completed_bool)[0],:seq_idx+1]\n",
    "        finished_logits[c_idx,:seq_idx+1]+=out_main[np.where(completed_bool)[0],:seq_idx+1]\n",
    "    logits=logits[(completed_bool==False).flatten()]\n",
    "    out_main=out_main[(completed_bool==False).flatten(),:,:]\n",
    "    print(f\"out_main after completed reeduce: {out_main.shape}\")\n",
    "    seq_idx+=1\n",
    "    print(f\"logits_shape: {logits.shape}\")\n",
    "    print(out.shape)\n",
    "    print(f\"####{seq_idx}:  completed_idx: {completed_idx}\")\n",
    "    # print(f\"####{seq_idx}:  \\n\\tincompleted_idx: {incompleted_idx} \\n\\tcompleted_idx:  {completed_idx} \\n\\t finished_probs: {finished_probs}\")\n",
    "    # out,attn=model(logits,encmemorylist=memory)\n",
    "    # logits.append(out[:,-1].unsqueeze(1))\n",
    "    # probs=F.log_softmax(out[:,-1],dim=-1)\n",
    "    # top_prob=probs.topk(1)[1]\n",
    "    \n",
    "print(finished_probs==target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_shape: torch.Size([6, 1]);completed_bool\n",
      "logits_shape: torch.Size([6, 1])\n",
      "####1:  \n",
      "\tincompleted_idx: [0 1 2 3 4 5] \n",
      "\tcompleted_idx:  [] \n",
      "\t finished_probs: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "logits_shape: torch.Size([5, 2]);completed_bool\n",
      "logits_shape: torch.Size([5, 2])\n",
      "####2:  \n",
      "\tincompleted_idx: [1 2 3 4 5] \n",
      "\tcompleted_idx:  [0] \n",
      "\t finished_probs: tensor([[101, 102,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0]])\n",
      "logits_shape: torch.Size([5, 3]);completed_bool\n",
      "logits_shape: torch.Size([5, 3])\n",
      "####3:  \n",
      "\tincompleted_idx: [1 2 3 4 5] \n",
      "\tcompleted_idx:  [0] \n",
      "\t finished_probs: tensor([[101, 102,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0]])\n",
      "logits_shape: torch.Size([5, 4]);completed_bool\n",
      "logits_shape: torch.Size([5, 4])\n",
      "####4:  \n",
      "\tincompleted_idx: [1 2 3 4 5] \n",
      "\tcompleted_idx:  [0] \n",
      "\t finished_probs: tensor([[101, 102,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0]])\n",
      "logits_shape: torch.Size([4, 5]);completed_bool\n",
      "logits_shape: torch.Size([4, 5])\n",
      "####5:  \n",
      "\tincompleted_idx: [2 3 4 5] \n",
      "\tcompleted_idx:  [0, 1] \n",
      "\t finished_probs: tensor([[  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "logits_shape: torch.Size([4, 6]);completed_bool\n",
      "logits_shape: torch.Size([4, 6])\n",
      "####6:  \n",
      "\tincompleted_idx: [2 3 4 5] \n",
      "\tcompleted_idx:  [0, 1] \n",
      "\t finished_probs: tensor([[  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "logits_shape: torch.Size([4, 7]);completed_bool\n",
      "logits_shape: torch.Size([4, 7])\n",
      "####7:  \n",
      "\tincompleted_idx: [2 3 4 5] \n",
      "\tcompleted_idx:  [0, 1] \n",
      "\t finished_probs: tensor([[  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "logits_shape: torch.Size([3, 8]);completed_bool\n",
      "logits_shape: torch.Size([3, 8])\n",
      "####8:  \n",
      "\tincompleted_idx: [3 4 5] \n",
      "\tcompleted_idx:  [0, 1, 2] \n",
      "\t finished_probs: tensor([[  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325, 21181, 10380, 12361,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "logits_shape: torch.Size([3, 9]);completed_bool\n",
      "logits_shape: torch.Size([3, 9])\n",
      "####9:  \n",
      "\tincompleted_idx: [3 4 5] \n",
      "\tcompleted_idx:  [0, 1, 2] \n",
      "\t finished_probs: tensor([[  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325, 21181, 10380, 12361,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "logits_shape: torch.Size([3, 10]);completed_bool\n",
      "logits_shape: torch.Size([3, 10])\n",
      "####10:  \n",
      "\tincompleted_idx: [3 4 5] \n",
      "\tcompleted_idx:  [0, 1, 2] \n",
      "\t finished_probs: tensor([[  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325, 21181, 10380, 12361,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "logits_shape: torch.Size([2, 11]);completed_bool\n",
      "logits_shape: torch.Size([2, 11])\n",
      "####11:  \n",
      "\tincompleted_idx: [4 5] \n",
      "\tcompleted_idx:  [0, 1, 2, 3] \n",
      "\t finished_probs: tensor([[  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325, 21181, 10380, 12361,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325, 21181, 10380, 13172,   117,   177, 21528,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "logits_shape: torch.Size([2, 12]);completed_bool\n",
      "logits_shape: torch.Size([2, 12])\n",
      "####12:  \n",
      "\tincompleted_idx: [4 5] \n",
      "\tcompleted_idx:  [0, 1, 2, 3] \n",
      "\t finished_probs: tensor([[  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325, 21181, 10380, 12361,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325, 21181, 10380, 13172,   117,   177, 21528,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "logits_shape: torch.Size([2, 13]);completed_bool\n",
      "logits_shape: torch.Size([2, 13])\n",
      "####13:  \n",
      "\tincompleted_idx: [4 5] \n",
      "\tcompleted_idx:  [0, 1, 2, 3] \n",
      "\t finished_probs: tensor([[  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325, 21181, 10380, 12361,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325, 21181, 10380, 13172,   117,   177, 21528,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "logits_shape: torch.Size([1, 14]);completed_bool\n",
      "logits_shape: torch.Size([1, 14])\n",
      "####14:  \n",
      "\tincompleted_idx: [5] \n",
      "\tcompleted_idx:  [0, 1, 2, 3, 4] \n",
      "\t finished_probs: tensor([[  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325, 21181, 10380, 12361,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325, 21181, 10380, 13172,   117,   177, 21528,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325, 21181, 10380, 13172,   117,   177, 21528,\n",
      "         10114, 69110, 10271,   102,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "logits_shape: torch.Size([1, 15]);completed_bool\n",
      "logits_shape: torch.Size([1, 15])\n",
      "####15:  \n",
      "\tincompleted_idx: [5] \n",
      "\tcompleted_idx:  [0, 1, 2, 3, 4] \n",
      "\t finished_probs: tensor([[  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325, 21181, 10380, 12361,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325, 21181, 10380, 13172,   117,   177, 21528,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325, 21181, 10380, 13172,   117,   177, 21528,\n",
      "         10114, 69110, 10271,   102,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "logits_shape: torch.Size([0, 16]);completed_bool\n",
      "logits_shape: torch.Size([0, 16])\n",
      "####16:  \n",
      "\tincompleted_idx: [] \n",
      "\tcompleted_idx:  [0, 1, 2, 3, 4, 5] \n",
      "\t finished_probs: tensor([[  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325, 21181, 10380, 12361,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325, 21181, 10380, 13172,   117,   177, 21528,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325, 21181, 10380, 13172,   117,   177, 21528,\n",
      "         10114, 69110, 10271,   102,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   177, 16138, 14325, 21181, 10380, 13172,   117,   177, 21528,\n",
      "         10114, 69110, 10271, 14234, 11940,   102,     0,     0,     0,     0]])\n",
      "tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "max_seql=20\n",
    "target=tokenizer(data,truncation=True,padding='max_length')\n",
    "batch_size=target.shape[0]\n",
    "logits_=logits=torch.tensor([tokenizer.vocab['[CLS]'] for _ in range(batch_size)]).reshape(-1,1)\n",
    "seq_idx=0\n",
    "completed_idx=[]\n",
    "incompleted_idx=np.arange(batch_size)\n",
    "finished_probs=torch.zeros(size=(batch_size,max_seql),dtype=torch.long)\n",
    "while True:\n",
    "    if logits.shape[1]==max_seql or incompleted_idx.size==0:\n",
    "        break\n",
    "    #out,attn=model(logits,encmemorylist=memory)\n",
    "    # logits.append(out[:,-1].unsqueeze(1))\n",
    "    # probs=F.log_softmax(out[:,-1],dim=-1)\n",
    "    # top_prob=probs.topk(1)[1]\n",
    "    top_prob=target[incompleted_idx,seq_idx].reshape(-1,1)\n",
    "    logits=torch.cat([logits,top_prob],axis=1) if seq_idx>0 else top_prob\n",
    "    completed_bool=(top_prob==tokenizer.vocab['[SEP]']).flatten()\n",
    "    c_idx=incompleted_idx[np.nonzero(completed_bool)]\n",
    "    c_idx=c_idx.tolist() if hasattr(c_idx,'__iter__') else [c_idx]\n",
    "    incompleted_idx=np.array([i for i in incompleted_idx if i not in c_idx])\n",
    "    if len(c_idx)>0:\n",
    "        completed_idx.extend(c_idx)\n",
    "        finished_probs[c_idx,:seq_idx+1]+=(logits[np.where(completed_bool)[0],:seq_idx+1]).int()\n",
    "    logits=logits[(completed_bool==False).flatten()]\n",
    "    print(f\"logits_shape: {logits.shape};completed_bool\")\n",
    "    seq_idx+=1\n",
    "    print(f\"logits_shape: {logits.shape}\")\n",
    "    print(f\"####{seq_idx}:  \\n\\tincompleted_idx: {incompleted_idx} \\n\\tcompleted_idx:  {completed_idx} \\n\\t finished_probs: {finished_probs}\")\n",
    "    #out,attn=model(logits,encmemorylist=memory)\n",
    "    # logits.append(out[:,-1].unsqueeze(1))\n",
    "    # probs=F.log_softmax(out[:,-1],dim=-1)\n",
    "    # top_prob=probs.topk(1)[1]\n",
    "    if seq_idx==10:\n",
    "            COMPBOOL=completed_bool\n",
    "            LOGIT=logits\n",
    "\n",
    "    \n",
    "print(finished_probs==target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mehedi/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (37 > 30). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated tokens from 31783 rows of data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mehedi/anaconda3/envs/hf_env/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from config import get_config\n",
    "class HFTokenizer:\n",
    "    def __init__(self,tokenizer_model,max_length):\n",
    "        tok_pth=os.path.join(\"data\",tokenizer_model.split('/')[-1],\"tokenizer\")\n",
    "        self.name=tokenizer_model\n",
    "        self.tokenizer=torch.hub.load('huggingface/pytorch-transformers', 'tokenizer',tokenizer_model) if not os.path.exists(tok_pth) else torch.hub.load('huggingface/pytorch-transformers', 'tokenizer',tok_pth)\n",
    "        self.save_pth=tok_pth\n",
    "        if not max_length:\n",
    "            max_length=(get_config())['max_seql']\n",
    "        self.tokenizer.model_max_length=max_length\n",
    "        self.vocab=self.tokenizer.vocab\n",
    "    def __call__(self,data,padding=None,truncation=True,return_tensors='pt',add_special_tokens=True):\n",
    "        return self.tokenizer(data,padding=padding,truncation=truncation,return_tensors=return_tensors,add_special_tokens=add_special_tokens).input_ids\n",
    "    \n",
    "    def decode(self,tok_ids):\n",
    "        return self.tokenizer.decode(tok_ids,skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokenizer)\n",
    "    \n",
    "    def save(self):\n",
    "        self.tokenizer.save_pretrained(self.save_pth)\n",
    "    \n",
    "    def update(self,data):\n",
    "        dpoints=0\n",
    "        if isinstance(data,torch.utils.data.dataloader.DataLoader):\n",
    "            for (img,text) in data:\n",
    "                self.tokenizer(text)\n",
    "                dpoints+=len(text)\n",
    "        elif isinstance(data,(np.ndarray,torch.Tensor)):\n",
    "            data=data.tolist()\n",
    "            self.tokenizer(data)\n",
    "            dpoints+=len(data)\n",
    "        else:\n",
    "            self.tokenizer(data)\n",
    "            dpoints+=len(data)\n",
    "        print(f\"Updated tokens from {dpoints} rows of data\")\n",
    "        self.save()\n",
    "\n",
    "class Stream_Dataset(Dataset):\n",
    "    def __init__(self,data_dir:str,max_seql:int=None,img_dir:str='',transforms=None,imgsz:int=300,csv_dir:str=None,tokenizer_model:Union[HFTokenizer,str]='medicalai/ClinicalBERT'):\n",
    "        self.data_dir=data_dir\n",
    "        csv_dir=os.path.join(data_dir,csv_dir) if csv_dir else self.get_default_csv(data_dir)\n",
    "        self.data=pd.read_csv(csv_dir).to_numpy()\n",
    "        self.img_dir=img_dir\n",
    "        self.imgsz=imgsz\n",
    "        self.resizer=torchvision.transforms.Resize(size=(imgsz,imgsz))\n",
    "        self.transforms=self.get_default_imgtransforms() if transforms=='default' else transforms\n",
    "        tokenizer=HFTokenizer(tokenizer_model,max_length=max_seql) if isinstance(tokenizer_model,str) else tokenizer_model\n",
    "        tokenizer.update(self.data[:,-1].tolist())\n",
    "        del tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img_name=self.data[idx,0]\n",
    "        semantic_str=self.data[idx,1].strip()\n",
    "        img=self.resizer(torchvision.io.read_image(self.data_dir+'/'+self.img_dir+'/'+img_name))\n",
    "        if self.transforms is not None:\n",
    "            img=self.transforms(img)\n",
    "        return img/255.,semantic_str\n",
    "    \n",
    "    def get_default_csv(self,data_dir):\n",
    "        csv_list=glob.glob(data_dir+\"/*.csv\")\n",
    "        largest_csv=sorted(csv_list,reverse=True,key=lambda x:len(pd.read_csv(x)))[0]\n",
    "        return largest_csv\n",
    "        \n",
    "    \n",
    "    def get_default_imgtransforms(self):\n",
    "        transforms_=torch.nn.Sequential(\n",
    "            transforms.RandomPosterize(bits=3,p=0.4),\n",
    "            transforms.RandomAdjustSharpness(sharpness_factor=0.5),\n",
    "            transforms.RandomRotation(degrees=(-10,20)),\n",
    "            transforms.ColorJitter(brightness=.2, hue=.1),\n",
    "            transforms.RandomAutocontrast()\n",
    "\n",
    "        )\n",
    "        return transforms_ \n",
    "\n",
    "dataset_=Stream_Dataset(\n",
    "        data_dir=\"/home/mehedi/Desktop/raghib/FLICKR-30K IMAGE CAPTIONING/flickr30k_images/\",\n",
    "        imgsz=300,\n",
    "        img_dir='flickr30k_images',\n",
    "        max_seql=30,\n",
    "        tokenizer_model='medicalai/ClinicalBERT',\n",
    "        csv_dir='result0.csv'\n",
    "\n",
    ")\n",
    "dataload=DataLoader(dataset=dataset_,batch_size=4)\n",
    "x,y=next(iter(dataload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory=[torch.rand([len(y),36,768]) for _ in range(2)]\n",
    "data=tokenizer(y,padding='max_length',truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 36, 768])"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.eval()\n",
    "decoder.evaluate(enc_memory_list=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   101,  10551,  14739,  75980,  12682,  10169,  48201,  99274,  10157,\n",
       "          40830,  25157,  10160,  10455,  27925,  11371,  60083,  10230,  10950,\n",
       "          10106,    102],\n",
       "        [   101,  11736,  10588,  10106,  19118,  11250,  10107,  10301,  24210,\n",
       "            169,  49429,  80870,  13005,  11787,    119,    102,      0,      0,\n",
       "              0,      0],\n",
       "        [   101,    169,  18048,  10106,    169,  78200,  67348,  10124, 106793,\n",
       "          10741,    169,  11847,  10108,  16527,  35819,  10106,  10151,  14722,\n",
       "          13170,    102],\n",
       "        [   101,  30455,  10106,    169,  23254,  81050,  10111,  11250,  10124,\n",
       "          32173,  10135,  16527,  10835,  10111,  20169,  11269,  11327,    169,\n",
       "          39051,    102]])"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mehedi/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "/home/mehedi/anaconda3/envs/hf_env/lib/python3.10/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]) torch.float32\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[365], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39m# encoder=SpatialStream(n_patches=36,img_chw=(3,300,300),activation=activation,num_ts_blocks=num_ts_blocks)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# decoder=SemanticStream(max_seql=45,d_model=768,num_ts_blocks=num_ts_blocks)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m----> 7\u001b[0m model(images\u001b[39m=\u001b[39;49mx,target\u001b[39m=\u001b[39;49my)\n",
      "File \u001b[0;32m~/anaconda3/envs/hf_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[364], line 153\u001b[0m, in \u001b[0;36mTwoStreamTransformer.forward\u001b[0;34m(self, images, target)\u001b[0m\n\u001b[1;32m    151\u001b[0m enc_memory_list\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(images)\n\u001b[1;32m    152\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m--> 153\u001b[0m     logits,attn\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder\u001b[39m.\u001b[39;49mevaluate(enc_memory_list\u001b[39m=\u001b[39;49menc_memory_list)\n\u001b[1;32m    154\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     logits,attn\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(enc_memory_list\u001b[39m=\u001b[39menc_memory_list,target\u001b[39m=\u001b[39mtarget)\n",
      "Cell \u001b[0;32mIn[364], line 134\u001b[0m, in \u001b[0;36mSemanticStream.evaluate\u001b[0;34m(self, enc_memory_list)\u001b[0m\n\u001b[1;32m    132\u001b[0m     seq_idx\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m    133\u001b[0m \u001b[39mprint\u001b[39m(finished_probs,finished_probs\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m--> 134\u001b[0m out,attn\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(target\u001b[39m=\u001b[39;49mfinished_probs,enc_memory_list\u001b[39m=\u001b[39;49menc_memory_list)\n\u001b[1;32m    135\u001b[0m \u001b[39mreturn\u001b[39;00m out,attn\n",
      "Cell \u001b[0;32mIn[364], line 84\u001b[0m, in \u001b[0;36mSemanticStream.forward\u001b[0;34m(self, target, enc_memory_list)\u001b[0m\n\u001b[1;32m     82\u001b[0m     tgt_padmask\u001b[39m=\u001b[39m(tgt\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     83\u001b[0m tgt_mask\u001b[39m=\u001b[39mgenerate_square_subsequent_mask(tgt\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m---> 84\u001b[0m embedded\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert_embedding(input_ids\u001b[39m=\u001b[39;49mtgt,attention_mask\u001b[39m=\u001b[39;49mtgt_padmask)\u001b[39m.\u001b[39mlast_hidden_state\n\u001b[1;32m     85\u001b[0m encoded\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_enc(embedded)\n\u001b[1;32m     86\u001b[0m \u001b[39mfor\u001b[39;00m idx,(decoder_layer,memory) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msemtransformer_blocks,enc_memory_list)):\n",
      "File \u001b[0;32m~/anaconda3/envs/hf_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/hf_env/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:581\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    579\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 581\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(input_ids, inputs_embeds)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer(\n\u001b[1;32m    584\u001b[0m     x\u001b[39m=\u001b[39membeddings,\n\u001b[1;32m    585\u001b[0m     attn_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    590\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/hf_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/hf_env/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:120\u001b[0m, in \u001b[0;36mEmbeddings.forward\u001b[0;34m(self, input_ids, input_embeds)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39mParameters:\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39m    input_ids (torch.Tensor):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39membeddings)\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     input_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mword_embeddings(input_ids)  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[1;32m    122\u001b[0m seq_length \u001b[39m=\u001b[39m input_embeds\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)\n\u001b[1;32m    124\u001b[0m \u001b[39m# Setting the position-ids to the registered buffer in constructor, it helps\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[39m# when tracing the model without passing position-ids, solves\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[39m# isues similar to issue #5664\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/hf_env/lib/python3.10/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/anaconda3/envs/hf_env/lib/python3.10/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "# activation='relu'\n",
    "# num_ts_blocks=2\n",
    "model=TwoStreamTransformer(n_patches=36,img_chw=(3,300,300),max_seql=45,activation=activation,num_ts_blocks=num_ts_blocks)\n",
    "# encoder=SpatialStream(n_patches=36,img_chw=(3,300,300),activation=activation,num_ts_blocks=num_ts_blocks)\n",
    "# decoder=SemanticStream(max_seql=45,d_model=768,num_ts_blocks=num_ts_blocks)\n",
    "model.eval()\n",
    "model(images=x,target=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 1078, in _pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\n",
      "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 297, in _pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\n",
      "  File \"/home/mehedi/anaconda3/envs/hf_env/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py\", line 1976, in do_wait_suspend\n",
      "    keep_suspended = self._do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n",
      "  File \"/home/mehedi/anaconda3/envs/hf_env/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py\", line 2011, in _do_wait_suspend\n",
      "    time.sleep(0.01)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[345], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      2\u001b[0m x,y\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mrand((\u001b[39m10\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m300\u001b[39m,\u001b[39m300\u001b[39m)),[\u001b[39m'\u001b[39m\u001b[39mi love chicken\u001b[39m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m)]\n\u001b[1;32m      3\u001b[0m logits,attn\u001b[39m=\u001b[39mmodel(images\u001b[39m=\u001b[39mx,target\u001b[39m=\u001b[39my)\n",
      "Cell \u001b[0;32mIn[345], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      2\u001b[0m x,y\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mrand((\u001b[39m10\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m300\u001b[39m,\u001b[39m300\u001b[39m)),[\u001b[39m'\u001b[39m\u001b[39mi love chicken\u001b[39m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m)]\n\u001b[1;32m      3\u001b[0m logits,attn\u001b[39m=\u001b[39mmodel(images\u001b[39m=\u001b[39mx,target\u001b[39m=\u001b[39my)\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1363\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:662\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1087\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1078\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:297\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf_env/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:1976\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   1973\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   1975\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001b[0;32m-> 1976\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   1978\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1980\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   1981\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf_env/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2011\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_mpl_hook()\n\u001b[1;32m   2010\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2011\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m   2013\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[1;32m   2015\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "x,y=torch.rand((10,3,300,300)),['i love chicken' for _ in range(10)]\n",
    "logits,attn=model(images=x,target=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 45, 119547])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mehedi/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "119547"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(load_tokenizer(tokenizer_model='medicalai/ClinicalBERT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010996341705322266\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "torch.tril(torch.ones(45,5))\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 45, 119547])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Users\\noorr\\anaconda3\\envs\\torch_\\lib\\site-packages\\torch\\nn\\functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   101,  95648],\n",
      "        [   101,  41026],\n",
      "        [   101,  45135],\n",
      "        [   101,  95305],\n",
      "        [   101,  45135],\n",
      "        [   101,  20901],\n",
      "        [   101,  95648],\n",
      "        [   101,  57118],\n",
      "        [   101,  45135],\n",
      "        [   101, 102133]]) ############################################################ torch.Size([10, 2])\n",
      "tensor([[   101,  95648,  86171],\n",
      "        [   101,  41026,  29440],\n",
      "        [   101,  45135,  31014],\n",
      "        [   101,  95305,  90805],\n",
      "        [   101,  45135,  67841],\n",
      "        [   101,  20901,  64118],\n",
      "        [   101,  95648,  31014],\n",
      "        [   101,  57118,  90805],\n",
      "        [   101,  45135,  96073],\n",
      "        [   101, 102133,  45135]]) ############################################################ torch.Size([10, 3])\n",
      "tensor([[   101,  95648,  86171,  45839],\n",
      "        [   101,  41026,  29440,  94638],\n",
      "        [   101,  45135,  31014,  37297],\n",
      "        [   101,  95305,  90805,  40710],\n",
      "        [   101,  45135,  67841,  22437],\n",
      "        [   101,  20901,  64118,  10321],\n",
      "        [   101,  95648,  31014,  86171],\n",
      "        [   101,  57118,  90805,  90805],\n",
      "        [   101,  45135,  96073,  94834],\n",
      "        [   101, 102133,  45135,  37297]]) ############################################################ torch.Size([10, 4])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214],\n",
      "        [   101,  41026,  29440,  94638,  94926],\n",
      "        [   101,  45135,  31014,  37297,  37297],\n",
      "        [   101,  95305,  90805,  40710, 112512],\n",
      "        [   101,  45135,  67841,  22437,  81516],\n",
      "        [   101,  20901,  64118,  10321,  86374],\n",
      "        [   101,  95648,  31014,  86171,  43770],\n",
      "        [   101,  57118,  90805,  90805,    984],\n",
      "        [   101,  45135,  96073,  94834,  52269],\n",
      "        [   101, 102133,  45135,  37297,  52460]]) ############################################################ torch.Size([10, 5])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848]]) ############################################################ torch.Size([10, 6])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553]]) ############################################################ torch.Size([10, 7])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206]]) ############################################################ torch.Size([10, 8])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484]]) ############################################################ torch.Size([10, 9])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484]]) ############################################################ torch.Size([10, 10])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606]]) ############################################################ torch.Size([10, 11])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553]]) ############################################################ torch.Size([10, 12])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553]]) ############################################################ torch.Size([10, 13])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212]]) ############################################################ torch.Size([10, 14])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979]]) ############################################################ torch.Size([10, 15])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866]]) ############################################################ torch.Size([10, 16])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541,   5919],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212,  17212],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275,  60849],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119,   7482],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212,  17212],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919, 112512],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949, 110710],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212,  17212],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212,  17212],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866,  61275]]) ############################################################ torch.Size([10, 17])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541,   5919,  32152],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212,  17212,  40377],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275,  60849,  17212],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119,   7482,  56759],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212,  17212,  34326],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919, 112512,  45315],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949, 110710,  99284],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212,  17212,  32152],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212,  17212,  17212],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866,  61275,  17212]]) ############################################################ torch.Size([10, 18])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541,   5919,  32152,\n",
      "          64499],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212,  17212,  40377,\n",
      "          40377],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275,  60849,  17212,\n",
      "          67839],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119,   7482,  56759,\n",
      "          32152],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212,  17212,  34326,\n",
      "          65793],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919, 112512,  45315,\n",
      "         101310],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949, 110710,  99284,\n",
      "          22812],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212,  17212,  32152,\n",
      "          66487],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212,  17212,  17212,\n",
      "          17212],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866,  61275,  17212,\n",
      "          17212]]) ############################################################ torch.Size([10, 19])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541,   5919,  32152,\n",
      "          64499, 100781],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212,  17212,  40377,\n",
      "          40377,  40377],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275,  60849,  17212,\n",
      "          67839,  89322],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119,   7482,  56759,\n",
      "          32152, 108297],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212,  17212,  34326,\n",
      "          65793,  17212],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919, 112512,  45315,\n",
      "         101310,  61275],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949, 110710,  99284,\n",
      "          22812,  44435],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212,  17212,  32152,\n",
      "          66487,  55500],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212,  17212,  17212,\n",
      "          17212,  92109],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866,  61275,  17212,\n",
      "          17212,  17212]]) ############################################################ torch.Size([10, 20])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541,   5919,  32152,\n",
      "          64499, 100781,  67190],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212,  17212,  40377,\n",
      "          40377,  40377,  24530],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275,  60849,  17212,\n",
      "          67839,  89322,  23686],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119,   7482,  56759,\n",
      "          32152, 108297,  44435],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212,  17212,  34326,\n",
      "          65793,  17212, 111785],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919, 112512,  45315,\n",
      "         101310,  61275,  81460],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949, 110710,  99284,\n",
      "          22812,  44435,  67190],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212,  17212,  32152,\n",
      "          66487,  55500,  66487],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212,  17212,  17212,\n",
      "          17212,  92109,  51651],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866,  61275,  17212,\n",
      "          17212,  17212,  92109]]) ############################################################ torch.Size([10, 21])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541,   5919,  32152,\n",
      "          64499, 100781,  67190,  44435],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212,  17212,  40377,\n",
      "          40377,  40377,  24530,  81460],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275,  60849,  17212,\n",
      "          67839,  89322,  23686,  90805],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119,   7482,  56759,\n",
      "          32152, 108297,  44435,  44435],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212,  17212,  34326,\n",
      "          65793,  17212, 111785,  39356],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919, 112512,  45315,\n",
      "         101310,  61275,  81460,  81460],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949, 110710,  99284,\n",
      "          22812,  44435,  67190,  56759],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212,  17212,  32152,\n",
      "          66487,  55500,  66487, 105182],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212,  17212,  17212,\n",
      "          17212,  92109,  51651,  55500],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866,  61275,  17212,\n",
      "          17212,  17212,  92109,  17212]]) ############################################################ torch.Size([10, 22])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541,   5919,  32152,\n",
      "          64499, 100781,  67190,  44435,  92109],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212,  17212,  40377,\n",
      "          40377,  40377,  24530,  81460,  61275],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275,  60849,  17212,\n",
      "          67839,  89322,  23686,  90805,  27157],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119,   7482,  56759,\n",
      "          32152, 108297,  44435,  44435,  44435],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212,  17212,  34326,\n",
      "          65793,  17212, 111785,  39356,  55500],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919, 112512,  45315,\n",
      "         101310,  61275,  81460,  81460, 114149],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949, 110710,  99284,\n",
      "          22812,  44435,  67190,  56759,  91784],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212,  17212,  32152,\n",
      "          66487,  55500,  66487, 105182,  14258],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212,  17212,  17212,\n",
      "          17212,  92109,  51651,  55500, 115582],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866,  61275,  17212,\n",
      "          17212,  17212,  92109,  17212,  90805]]) ############################################################ torch.Size([10, 23])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541,   5919,  32152,\n",
      "          64499, 100781,  67190,  44435,  92109,  82606],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212,  17212,  40377,\n",
      "          40377,  40377,  24530,  81460,  61275, 112504],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275,  60849,  17212,\n",
      "          67839,  89322,  23686,  90805,  27157,  80257],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119,   7482,  56759,\n",
      "          32152, 108297,  44435,  44435,  44435, 103964],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212,  17212,  34326,\n",
      "          65793,  17212, 111785,  39356,  55500,  50335],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919, 112512,  45315,\n",
      "         101310,  61275,  81460,  81460, 114149,  78406],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949, 110710,  99284,\n",
      "          22812,  44435,  67190,  56759,  91784,  63357],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212,  17212,  32152,\n",
      "          66487,  55500,  66487, 105182,  14258,  51309],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212,  17212,  17212,\n",
      "          17212,  92109,  51651,  55500, 115582,  55500],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866,  61275,  17212,\n",
      "          17212,  17212,  92109,  17212,  90805,  64118]]) ############################################################ torch.Size([10, 24])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541,   5919,  32152,\n",
      "          64499, 100781,  67190,  44435,  92109,  82606,  64499],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212,  17212,  40377,\n",
      "          40377,  40377,  24530,  81460,  61275, 112504, 101310],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275,  60849,  17212,\n",
      "          67839,  89322,  23686,  90805,  27157,  80257,  97256],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119,   7482,  56759,\n",
      "          32152, 108297,  44435,  44435,  44435, 103964,  44435],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212,  17212,  34326,\n",
      "          65793,  17212, 111785,  39356,  55500,  50335,   4218],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919, 112512,  45315,\n",
      "         101310,  61275,  81460,  81460, 114149,  78406,  92109],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949, 110710,  99284,\n",
      "          22812,  44435,  67190,  56759,  91784,  63357,  39356],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212,  17212,  32152,\n",
      "          66487,  55500,  66487, 105182,  14258,  51309,  49541],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212,  17212,  17212,\n",
      "          17212,  92109,  51651,  55500, 115582,  55500,  64118],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866,  61275,  17212,\n",
      "          17212,  17212,  92109,  17212,  90805,  64118,  20036]]) ############################################################ torch.Size([10, 25])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541,   5919,  32152,\n",
      "          64499, 100781,  67190,  44435,  92109,  82606,  64499,  64499],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212,  17212,  40377,\n",
      "          40377,  40377,  24530,  81460,  61275, 112504, 101310,  32152],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275,  60849,  17212,\n",
      "          67839,  89322,  23686,  90805,  27157,  80257,  97256,  44435],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119,   7482,  56759,\n",
      "          32152, 108297,  44435,  44435,  44435, 103964,  44435,  61275],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212,  17212,  34326,\n",
      "          65793,  17212, 111785,  39356,  55500,  50335,   4218,  39356],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919, 112512,  45315,\n",
      "         101310,  61275,  81460,  81460, 114149,  78406,  92109,  46196],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949, 110710,  99284,\n",
      "          22812,  44435,  67190,  56759,  91784,  63357,  39356, 112512],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212,  17212,  32152,\n",
      "          66487,  55500,  66487, 105182,  14258,  51309,  49541,  73450],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212,  17212,  17212,\n",
      "          17212,  92109,  51651,  55500, 115582,  55500,  64118,  19609],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866,  61275,  17212,\n",
      "          17212,  17212,  92109,  17212,  90805,  64118,  20036,  82606]]) ############################################################ torch.Size([10, 26])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541,   5919,  32152,\n",
      "          64499, 100781,  67190,  44435,  92109,  82606,  64499,  64499,  61275],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212,  17212,  40377,\n",
      "          40377,  40377,  24530,  81460,  61275, 112504, 101310,  32152,  85879],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275,  60849,  17212,\n",
      "          67839,  89322,  23686,  90805,  27157,  80257,  97256,  44435,  27157],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119,   7482,  56759,\n",
      "          32152, 108297,  44435,  44435,  44435, 103964,  44435,  61275,  17212],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212,  17212,  34326,\n",
      "          65793,  17212, 111785,  39356,  55500,  50335,   4218,  39356,  82606],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919, 112512,  45315,\n",
      "         101310,  61275,  81460,  81460, 114149,  78406,  92109,  46196,  34326],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949, 110710,  99284,\n",
      "          22812,  44435,  67190,  56759,  91784,  63357,  39356, 112512,  82606],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212,  17212,  32152,\n",
      "          66487,  55500,  66487, 105182,  14258,  51309,  49541,  73450,  90805],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212,  17212,  17212,\n",
      "          17212,  92109,  51651,  55500, 115582,  55500,  64118,  19609,  28163],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866,  61275,  17212,\n",
      "          17212,  17212,  92109,  17212,  90805,  64118,  20036,  82606,  85879]]) ############################################################ torch.Size([10, 27])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541,   5919,  32152,\n",
      "          64499, 100781,  67190,  44435,  92109,  82606,  64499,  64499,  61275,\n",
      "          82606],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212,  17212,  40377,\n",
      "          40377,  40377,  24530,  81460,  61275, 112504, 101310,  32152,  85879,\n",
      "          17212],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275,  60849,  17212,\n",
      "          67839,  89322,  23686,  90805,  27157,  80257,  97256,  44435,  27157,\n",
      "         106066],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119,   7482,  56759,\n",
      "          32152, 108297,  44435,  44435,  44435, 103964,  44435,  61275,  17212,\n",
      "          43258],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212,  17212,  34326,\n",
      "          65793,  17212, 111785,  39356,  55500,  50335,   4218,  39356,  82606,\n",
      "          82606],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919, 112512,  45315,\n",
      "         101310,  61275,  81460,  81460, 114149,  78406,  92109,  46196,  34326,\n",
      "          82606],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949, 110710,  99284,\n",
      "          22812,  44435,  67190,  56759,  91784,  63357,  39356, 112512,  82606,\n",
      "          89568],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212,  17212,  32152,\n",
      "          66487,  55500,  66487, 105182,  14258,  51309,  49541,  73450,  90805,\n",
      "          82606],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212,  17212,  17212,\n",
      "          17212,  92109,  51651,  55500, 115582,  55500,  64118,  19609,  28163,\n",
      "         106066],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866,  61275,  17212,\n",
      "          17212,  17212,  92109,  17212,  90805,  64118,  20036,  82606,  85879,\n",
      "          82606]]) ############################################################ torch.Size([10, 28])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541,   5919,  32152,\n",
      "          64499, 100781,  67190,  44435,  92109,  82606,  64499,  64499,  61275,\n",
      "          82606,  34326],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212,  17212,  40377,\n",
      "          40377,  40377,  24530,  81460,  61275, 112504, 101310,  32152,  85879,\n",
      "          17212,  67839],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275,  60849,  17212,\n",
      "          67839,  89322,  23686,  90805,  27157,  80257,  97256,  44435,  27157,\n",
      "         106066,  27157],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119,   7482,  56759,\n",
      "          32152, 108297,  44435,  44435,  44435, 103964,  44435,  61275,  17212,\n",
      "          43258, 106066],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212,  17212,  34326,\n",
      "          65793,  17212, 111785,  39356,  55500,  50335,   4218,  39356,  82606,\n",
      "          82606,  82606],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919, 112512,  45315,\n",
      "         101310,  61275,  81460,  81460, 114149,  78406,  92109,  46196,  34326,\n",
      "          82606, 113412],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949, 110710,  99284,\n",
      "          22812,  44435,  67190,  56759,  91784,  63357,  39356, 112512,  82606,\n",
      "          89568,  89568],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212,  17212,  32152,\n",
      "          66487,  55500,  66487, 105182,  14258,  51309,  49541,  73450,  90805,\n",
      "          82606,  34326],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212,  17212,  17212,\n",
      "          17212,  92109,  51651,  55500, 115582,  55500,  64118,  19609,  28163,\n",
      "         106066,  53949],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866,  61275,  17212,\n",
      "          17212,  17212,  92109,  17212,  90805,  64118,  20036,  82606,  85879,\n",
      "          82606,  90805]]) ############################################################ torch.Size([10, 29])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541,   5919,  32152,\n",
      "          64499, 100781,  67190,  44435,  92109,  82606,  64499,  64499,  61275,\n",
      "          82606,  34326,   2157],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212,  17212,  40377,\n",
      "          40377,  40377,  24530,  81460,  61275, 112504, 101310,  32152,  85879,\n",
      "          17212,  67839, 113529],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275,  60849,  17212,\n",
      "          67839,  89322,  23686,  90805,  27157,  80257,  97256,  44435,  27157,\n",
      "         106066,  27157,  66771],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119,   7482,  56759,\n",
      "          32152, 108297,  44435,  44435,  44435, 103964,  44435,  61275,  17212,\n",
      "          43258, 106066, 106066],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212,  17212,  34326,\n",
      "          65793,  17212, 111785,  39356,  55500,  50335,   4218,  39356,  82606,\n",
      "          82606,  82606,  55659],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919, 112512,  45315,\n",
      "         101310,  61275,  81460,  81460, 114149,  78406,  92109,  46196,  34326,\n",
      "          82606, 113412,  89934],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949, 110710,  99284,\n",
      "          22812,  44435,  67190,  56759,  91784,  63357,  39356, 112512,  82606,\n",
      "          89568,  89568,  89934],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212,  17212,  32152,\n",
      "          66487,  55500,  66487, 105182,  14258,  51309,  49541,  73450,  90805,\n",
      "          82606,  34326, 113529],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212,  17212,  17212,\n",
      "          17212,  92109,  51651,  55500, 115582,  55500,  64118,  19609,  28163,\n",
      "         106066,  53949, 111108],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866,  61275,  17212,\n",
      "          17212,  17212,  92109,  17212,  90805,  64118,  20036,  82606,  85879,\n",
      "          82606,  90805,  17212]]) ############################################################ torch.Size([10, 30])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541,   5919,  32152,\n",
      "          64499, 100781,  67190,  44435,  92109,  82606,  64499,  64499,  61275,\n",
      "          82606,  34326,   2157,   2496],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212,  17212,  40377,\n",
      "          40377,  40377,  24530,  81460,  61275, 112504, 101310,  32152,  85879,\n",
      "          17212,  67839, 113529,  89934],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275,  60849,  17212,\n",
      "          67839,  89322,  23686,  90805,  27157,  80257,  97256,  44435,  27157,\n",
      "         106066,  27157,  66771, 112512],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119,   7482,  56759,\n",
      "          32152, 108297,  44435,  44435,  44435, 103964,  44435,  61275,  17212,\n",
      "          43258, 106066, 106066,  38028],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212,  17212,  34326,\n",
      "          65793,  17212, 111785,  39356,  55500,  50335,   4218,  39356,  82606,\n",
      "          82606,  82606,  55659,  50272],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919, 112512,  45315,\n",
      "         101310,  61275,  81460,  81460, 114149,  78406,  92109,  46196,  34326,\n",
      "          82606, 113412,  89934,  63545],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949, 110710,  99284,\n",
      "          22812,  44435,  67190,  56759,  91784,  63357,  39356, 112512,  82606,\n",
      "          89568,  89568,  89934,  89934],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212,  17212,  32152,\n",
      "          66487,  55500,  66487, 105182,  14258,  51309,  49541,  73450,  90805,\n",
      "          82606,  34326, 113529,  21911],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212,  17212,  17212,\n",
      "          17212,  92109,  51651,  55500, 115582,  55500,  64118,  19609,  28163,\n",
      "         106066,  53949, 111108,  89934],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866,  61275,  17212,\n",
      "          17212,  17212,  92109,  17212,  90805,  64118,  20036,  82606,  85879,\n",
      "          82606,  90805,  17212,  82606]]) ############################################################ torch.Size([10, 31])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541,   5919,  32152,\n",
      "          64499, 100781,  67190,  44435,  92109,  82606,  64499,  64499,  61275,\n",
      "          82606,  34326,   2157,   2496,  61275],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212,  17212,  40377,\n",
      "          40377,  40377,  24530,  81460,  61275, 112504, 101310,  32152,  85879,\n",
      "          17212,  67839, 113529,  89934,  67839],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275,  60849,  17212,\n",
      "          67839,  89322,  23686,  90805,  27157,  80257,  97256,  44435,  27157,\n",
      "         106066,  27157,  66771, 112512,  21911],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119,   7482,  56759,\n",
      "          32152, 108297,  44435,  44435,  44435, 103964,  44435,  61275,  17212,\n",
      "          43258, 106066, 106066,  38028,  89934],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212,  17212,  34326,\n",
      "          65793,  17212, 111785,  39356,  55500,  50335,   4218,  39356,  82606,\n",
      "          82606,  82606,  55659,  50272,  67839],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919, 112512,  45315,\n",
      "         101310,  61275,  81460,  81460, 114149,  78406,  92109,  46196,  34326,\n",
      "          82606, 113412,  89934,  63545, 112512],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949, 110710,  99284,\n",
      "          22812,  44435,  67190,  56759,  91784,  63357,  39356, 112512,  82606,\n",
      "          89568,  89568,  89934,  89934, 112512],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212,  17212,  32152,\n",
      "          66487,  55500,  66487, 105182,  14258,  51309,  49541,  73450,  90805,\n",
      "          82606,  34326, 113529,  21911, 112512],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212,  17212,  17212,\n",
      "          17212,  92109,  51651,  55500, 115582,  55500,  64118,  19609,  28163,\n",
      "         106066,  53949, 111108,  89934,  67839],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866,  61275,  17212,\n",
      "          17212,  17212,  92109,  17212,  90805,  64118,  20036,  82606,  85879,\n",
      "          82606,  90805,  17212,  82606,  67937]]) ############################################################ torch.Size([10, 32])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541,   5919,  32152,\n",
      "          64499, 100781,  67190,  44435,  92109,  82606,  64499,  64499,  61275,\n",
      "          82606,  34326,   2157,   2496,  61275,  90925],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212,  17212,  40377,\n",
      "          40377,  40377,  24530,  81460,  61275, 112504, 101310,  32152,  85879,\n",
      "          17212,  67839, 113529,  89934,  67839, 114149],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275,  60849,  17212,\n",
      "          67839,  89322,  23686,  90805,  27157,  80257,  97256,  44435,  27157,\n",
      "         106066,  27157,  66771, 112512,  21911, 112512],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119,   7482,  56759,\n",
      "          32152, 108297,  44435,  44435,  44435, 103964,  44435,  61275,  17212,\n",
      "          43258, 106066, 106066,  38028,  89934,  21911],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212,  17212,  34326,\n",
      "          65793,  17212, 111785,  39356,  55500,  50335,   4218,  39356,  82606,\n",
      "          82606,  82606,  55659,  50272,  67839,  21911],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919, 112512,  45315,\n",
      "         101310,  61275,  81460,  81460, 114149,  78406,  92109,  46196,  34326,\n",
      "          82606, 113412,  89934,  63545, 112512,  24324],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949, 110710,  99284,\n",
      "          22812,  44435,  67190,  56759,  91784,  63357,  39356, 112512,  82606,\n",
      "          89568,  89568,  89934,  89934, 112512,  67839],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212,  17212,  32152,\n",
      "          66487,  55500,  66487, 105182,  14258,  51309,  49541,  73450,  90805,\n",
      "          82606,  34326, 113529,  21911, 112512,  21911],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212,  17212,  17212,\n",
      "          17212,  92109,  51651,  55500, 115582,  55500,  64118,  19609,  28163,\n",
      "         106066,  53949, 111108,  89934,  67839,    317],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866,  61275,  17212,\n",
      "          17212,  17212,  92109,  17212,  90805,  64118,  20036,  82606,  85879,\n",
      "          82606,  90805,  17212,  82606,  67937,    317]]) ############################################################ torch.Size([10, 33])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541,   5919,  32152,\n",
      "          64499, 100781,  67190,  44435,  92109,  82606,  64499,  64499,  61275,\n",
      "          82606,  34326,   2157,   2496,  61275,  90925, 112512],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212,  17212,  40377,\n",
      "          40377,  40377,  24530,  81460,  61275, 112504, 101310,  32152,  85879,\n",
      "          17212,  67839, 113529,  89934,  67839, 114149,  81460],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275,  60849,  17212,\n",
      "          67839,  89322,  23686,  90805,  27157,  80257,  97256,  44435,  27157,\n",
      "         106066,  27157,  66771, 112512,  21911, 112512,     98],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119,   7482,  56759,\n",
      "          32152, 108297,  44435,  44435,  44435, 103964,  44435,  61275,  17212,\n",
      "          43258, 106066, 106066,  38028,  89934,  21911,   8505],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212,  17212,  34326,\n",
      "          65793,  17212, 111785,  39356,  55500,  50335,   4218,  39356,  82606,\n",
      "          82606,  82606,  55659,  50272,  67839,  21911,  64499],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919, 112512,  45315,\n",
      "         101310,  61275,  81460,  81460, 114149,  78406,  92109,  46196,  34326,\n",
      "          82606, 113412,  89934,  63545, 112512,  24324,  24324],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949, 110710,  99284,\n",
      "          22812,  44435,  67190,  56759,  91784,  63357,  39356, 112512,  82606,\n",
      "          89568,  89568,  89934,  89934, 112512,  67839,  89934],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212,  17212,  32152,\n",
      "          66487,  55500,  66487, 105182,  14258,  51309,  49541,  73450,  90805,\n",
      "          82606,  34326, 113529,  21911, 112512,  21911,  28834],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212,  17212,  17212,\n",
      "          17212,  92109,  51651,  55500, 115582,  55500,  64118,  19609,  28163,\n",
      "         106066,  53949, 111108,  89934,  67839,    317,  67839],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866,  61275,  17212,\n",
      "          17212,  17212,  92109,  17212,  90805,  64118,  20036,  82606,  85879,\n",
      "          82606,  90805,  17212,  82606,  67937,    317,  65496]]) ############################################################ torch.Size([10, 34])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541,   5919,  32152,\n",
      "          64499, 100781,  67190,  44435,  92109,  82606,  64499,  64499,  61275,\n",
      "          82606,  34326,   2157,   2496,  61275,  90925, 112512,  92109],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212,  17212,  40377,\n",
      "          40377,  40377,  24530,  81460,  61275, 112504, 101310,  32152,  85879,\n",
      "          17212,  67839, 113529,  89934,  67839, 114149,  81460,  53949],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275,  60849,  17212,\n",
      "          67839,  89322,  23686,  90805,  27157,  80257,  97256,  44435,  27157,\n",
      "         106066,  27157,  66771, 112512,  21911, 112512,     98,  51214],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119,   7482,  56759,\n",
      "          32152, 108297,  44435,  44435,  44435, 103964,  44435,  61275,  17212,\n",
      "          43258, 106066, 106066,  38028,  89934,  21911,   8505,    317],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212,  17212,  34326,\n",
      "          65793,  17212, 111785,  39356,  55500,  50335,   4218,  39356,  82606,\n",
      "          82606,  82606,  55659,  50272,  67839,  21911,  64499,  66623],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919, 112512,  45315,\n",
      "         101310,  61275,  81460,  81460, 114149,  78406,  92109,  46196,  34326,\n",
      "          82606, 113412,  89934,  63545, 112512,  24324,  24324,   3339],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949, 110710,  99284,\n",
      "          22812,  44435,  67190,  56759,  91784,  63357,  39356, 112512,  82606,\n",
      "          89568,  89568,  89934,  89934, 112512,  67839,  89934, 111439],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212,  17212,  32152,\n",
      "          66487,  55500,  66487, 105182,  14258,  51309,  49541,  73450,  90805,\n",
      "          82606,  34326, 113529,  21911, 112512,  21911,  28834,   3339],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212,  17212,  17212,\n",
      "          17212,  92109,  51651,  55500, 115582,  55500,  64118,  19609,  28163,\n",
      "         106066,  53949, 111108,  89934,  67839,    317,  67839,  59929],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866,  61275,  17212,\n",
      "          17212,  17212,  92109,  17212,  90805,  64118,  20036,  82606,  85879,\n",
      "          82606,  90805,  17212,  82606,  67937,    317,  65496,  87259]]) ############################################################ torch.Size([10, 35])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541,   5919,  32152,\n",
      "          64499, 100781,  67190,  44435,  92109,  82606,  64499,  64499,  61275,\n",
      "          82606,  34326,   2157,   2496,  61275,  90925, 112512,  92109, 106066],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212,  17212,  40377,\n",
      "          40377,  40377,  24530,  81460,  61275, 112504, 101310,  32152,  85879,\n",
      "          17212,  67839, 113529,  89934,  67839, 114149,  81460,  53949,  67839],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275,  60849,  17212,\n",
      "          67839,  89322,  23686,  90805,  27157,  80257,  97256,  44435,  27157,\n",
      "         106066,  27157,  66771, 112512,  21911, 112512,     98,  51214,  13448],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119,   7482,  56759,\n",
      "          32152, 108297,  44435,  44435,  44435, 103964,  44435,  61275,  17212,\n",
      "          43258, 106066, 106066,  38028,  89934,  21911,   8505,    317,  99284],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212,  17212,  34326,\n",
      "          65793,  17212, 111785,  39356,  55500,  50335,   4218,  39356,  82606,\n",
      "          82606,  82606,  55659,  50272,  67839,  21911,  64499,  66623,  81460],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919, 112512,  45315,\n",
      "         101310,  61275,  81460,  81460, 114149,  78406,  92109,  46196,  34326,\n",
      "          82606, 113412,  89934,  63545, 112512,  24324,  24324,   3339, 103964],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949, 110710,  99284,\n",
      "          22812,  44435,  67190,  56759,  91784,  63357,  39356, 112512,  82606,\n",
      "          89568,  89568,  89934,  89934, 112512,  67839,  89934, 111439, 104059],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212,  17212,  32152,\n",
      "          66487,  55500,  66487, 105182,  14258,  51309,  49541,  73450,  90805,\n",
      "          82606,  34326, 113529,  21911, 112512,  21911,  28834,   3339,  93635],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212,  17212,  17212,\n",
      "          17212,  92109,  51651,  55500, 115582,  55500,  64118,  19609,  28163,\n",
      "         106066,  53949, 111108,  89934,  67839,    317,  67839,  59929,  65526],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866,  61275,  17212,\n",
      "          17212,  17212,  92109,  17212,  90805,  64118,  20036,  82606,  85879,\n",
      "          82606,  90805,  17212,  82606,  67937,    317,  65496,  87259,  40377]]) ############################################################ torch.Size([10, 36])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541,   5919,  32152,\n",
      "          64499, 100781,  67190,  44435,  92109,  82606,  64499,  64499,  61275,\n",
      "          82606,  34326,   2157,   2496,  61275,  90925, 112512,  92109, 106066,\n",
      "          55500],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212,  17212,  40377,\n",
      "          40377,  40377,  24530,  81460,  61275, 112504, 101310,  32152,  85879,\n",
      "          17212,  67839, 113529,  89934,  67839, 114149,  81460,  53949,  67839,\n",
      "          81460],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275,  60849,  17212,\n",
      "          67839,  89322,  23686,  90805,  27157,  80257,  97256,  44435,  27157,\n",
      "         106066,  27157,  66771, 112512,  21911, 112512,     98,  51214,  13448,\n",
      "         108768],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119,   7482,  56759,\n",
      "          32152, 108297,  44435,  44435,  44435, 103964,  44435,  61275,  17212,\n",
      "          43258, 106066, 106066,  38028,  89934,  21911,   8505,    317,  99284,\n",
      "          27157],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212,  17212,  34326,\n",
      "          65793,  17212, 111785,  39356,  55500,  50335,   4218,  39356,  82606,\n",
      "          82606,  82606,  55659,  50272,  67839,  21911,  64499,  66623,  81460,\n",
      "          40377],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919, 112512,  45315,\n",
      "         101310,  61275,  81460,  81460, 114149,  78406,  92109,  46196,  34326,\n",
      "          82606, 113412,  89934,  63545, 112512,  24324,  24324,   3339, 103964,\n",
      "          51309],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949, 110710,  99284,\n",
      "          22812,  44435,  67190,  56759,  91784,  63357,  39356, 112512,  82606,\n",
      "          89568,  89568,  89934,  89934, 112512,  67839,  89934, 111439, 104059,\n",
      "          89934],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212,  17212,  32152,\n",
      "          66487,  55500,  66487, 105182,  14258,  51309,  49541,  73450,  90805,\n",
      "          82606,  34326, 113529,  21911, 112512,  21911,  28834,   3339,  93635,\n",
      "          34506],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212,  17212,  17212,\n",
      "          17212,  92109,  51651,  55500, 115582,  55500,  64118,  19609,  28163,\n",
      "         106066,  53949, 111108,  89934,  67839,    317,  67839,  59929,  65526,\n",
      "          69037],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866,  61275,  17212,\n",
      "          17212,  17212,  92109,  17212,  90805,  64118,  20036,  82606,  85879,\n",
      "          82606,  90805,  17212,  82606,  67937,    317,  65496,  87259,  40377,\n",
      "          74667]]) ############################################################ torch.Size([10, 37])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541,   5919,  32152,\n",
      "          64499, 100781,  67190,  44435,  92109,  82606,  64499,  64499,  61275,\n",
      "          82606,  34326,   2157,   2496,  61275,  90925, 112512,  92109, 106066,\n",
      "          55500,  68281],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212,  17212,  40377,\n",
      "          40377,  40377,  24530,  81460,  61275, 112504, 101310,  32152,  85879,\n",
      "          17212,  67839, 113529,  89934,  67839, 114149,  81460,  53949,  67839,\n",
      "          81460,  81460],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275,  60849,  17212,\n",
      "          67839,  89322,  23686,  90805,  27157,  80257,  97256,  44435,  27157,\n",
      "         106066,  27157,  66771, 112512,  21911, 112512,     98,  51214,  13448,\n",
      "         108768,  51309],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119,   7482,  56759,\n",
      "          32152, 108297,  44435,  44435,  44435, 103964,  44435,  61275,  17212,\n",
      "          43258, 106066, 106066,  38028,  89934,  21911,   8505,    317,  99284,\n",
      "          27157,  92109],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212,  17212,  34326,\n",
      "          65793,  17212, 111785,  39356,  55500,  50335,   4218,  39356,  82606,\n",
      "          82606,  82606,  55659,  50272,  67839,  21911,  64499,  66623,  81460,\n",
      "          40377, 111439],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919, 112512,  45315,\n",
      "         101310,  61275,  81460,  81460, 114149,  78406,  92109,  46196,  34326,\n",
      "          82606, 113412,  89934,  63545, 112512,  24324,  24324,   3339, 103964,\n",
      "          51309, 108768],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949, 110710,  99284,\n",
      "          22812,  44435,  67190,  56759,  91784,  63357,  39356, 112512,  82606,\n",
      "          89568,  89568,  89934,  89934, 112512,  67839,  89934, 111439, 104059,\n",
      "          89934, 104059],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212,  17212,  32152,\n",
      "          66487,  55500,  66487, 105182,  14258,  51309,  49541,  73450,  90805,\n",
      "          82606,  34326, 113529,  21911, 112512,  21911,  28834,   3339,  93635,\n",
      "          34506,  51309],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212,  17212,  17212,\n",
      "          17212,  92109,  51651,  55500, 115582,  55500,  64118,  19609,  28163,\n",
      "         106066,  53949, 111108,  89934,  67839,    317,  67839,  59929,  65526,\n",
      "          69037,  34506],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866,  61275,  17212,\n",
      "          17212,  17212,  92109,  17212,  90805,  64118,  20036,  82606,  85879,\n",
      "          82606,  90805,  17212,  82606,  67937,    317,  65496,  87259,  40377,\n",
      "          74667,  85879]]) ############################################################ torch.Size([10, 38])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541,   5919,  32152,\n",
      "          64499, 100781,  67190,  44435,  92109,  82606,  64499,  64499,  61275,\n",
      "          82606,  34326,   2157,   2496,  61275,  90925, 112512,  92109, 106066,\n",
      "          55500,  68281,  53949],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212,  17212,  40377,\n",
      "          40377,  40377,  24530,  81460,  61275, 112504, 101310,  32152,  85879,\n",
      "          17212,  67839, 113529,  89934,  67839, 114149,  81460,  53949,  67839,\n",
      "          81460,  81460,  95542],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275,  60849,  17212,\n",
      "          67839,  89322,  23686,  90805,  27157,  80257,  97256,  44435,  27157,\n",
      "         106066,  27157,  66771, 112512,  21911, 112512,     98,  51214,  13448,\n",
      "         108768,  51309,  85879],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119,   7482,  56759,\n",
      "          32152, 108297,  44435,  44435,  44435, 103964,  44435,  61275,  17212,\n",
      "          43258, 106066, 106066,  38028,  89934,  21911,   8505,    317,  99284,\n",
      "          27157,  92109, 103553],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212,  17212,  34326,\n",
      "          65793,  17212, 111785,  39356,  55500,  50335,   4218,  39356,  82606,\n",
      "          82606,  82606,  55659,  50272,  67839,  21911,  64499,  66623,  81460,\n",
      "          40377, 111439, 111439],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919, 112512,  45315,\n",
      "         101310,  61275,  81460,  81460, 114149,  78406,  92109,  46196,  34326,\n",
      "          82606, 113412,  89934,  63545, 112512,  24324,  24324,   3339, 103964,\n",
      "          51309, 108768, 103553],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949, 110710,  99284,\n",
      "          22812,  44435,  67190,  56759,  91784,  63357,  39356, 112512,  82606,\n",
      "          89568,  89568,  89934,  89934, 112512,  67839,  89934, 111439, 104059,\n",
      "          89934, 104059,  85879],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212,  17212,  32152,\n",
      "          66487,  55500,  66487, 105182,  14258,  51309,  49541,  73450,  90805,\n",
      "          82606,  34326, 113529,  21911, 112512,  21911,  28834,   3339,  93635,\n",
      "          34506,  51309,  85879],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212,  17212,  17212,\n",
      "          17212,  92109,  51651,  55500, 115582,  55500,  64118,  19609,  28163,\n",
      "         106066,  53949, 111108,  89934,  67839,    317,  67839,  59929,  65526,\n",
      "          69037,  34506,  85879],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866,  61275,  17212,\n",
      "          17212,  17212,  92109,  17212,  90805,  64118,  20036,  82606,  85879,\n",
      "          82606,  90805,  17212,  82606,  67937,    317,  65496,  87259,  40377,\n",
      "          74667,  85879,  51309]]) ############################################################ torch.Size([10, 39])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541,   5919,  32152,\n",
      "          64499, 100781,  67190,  44435,  92109,  82606,  64499,  64499,  61275,\n",
      "          82606,  34326,   2157,   2496,  61275,  90925, 112512,  92109, 106066,\n",
      "          55500,  68281,  53949,  64499],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212,  17212,  40377,\n",
      "          40377,  40377,  24530,  81460,  61275, 112504, 101310,  32152,  85879,\n",
      "          17212,  67839, 113529,  89934,  67839, 114149,  81460,  53949,  67839,\n",
      "          81460,  81460,  95542,  64499],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275,  60849,  17212,\n",
      "          67839,  89322,  23686,  90805,  27157,  80257,  97256,  44435,  27157,\n",
      "         106066,  27157,  66771, 112512,  21911, 112512,     98,  51214,  13448,\n",
      "         108768,  51309,  85879,  68281],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119,   7482,  56759,\n",
      "          32152, 108297,  44435,  44435,  44435, 103964,  44435,  61275,  17212,\n",
      "          43258, 106066, 106066,  38028,  89934,  21911,   8505,    317,  99284,\n",
      "          27157,  92109, 103553,  68281],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212,  17212,  34326,\n",
      "          65793,  17212, 111785,  39356,  55500,  50335,   4218,  39356,  82606,\n",
      "          82606,  82606,  55659,  50272,  67839,  21911,  64499,  66623,  81460,\n",
      "          40377, 111439, 111439,  90635],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919, 112512,  45315,\n",
      "         101310,  61275,  81460,  81460, 114149,  78406,  92109,  46196,  34326,\n",
      "          82606, 113412,  89934,  63545, 112512,  24324,  24324,   3339, 103964,\n",
      "          51309, 108768, 103553,  64499],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949, 110710,  99284,\n",
      "          22812,  44435,  67190,  56759,  91784,  63357,  39356, 112512,  82606,\n",
      "          89568,  89568,  89934,  89934, 112512,  67839,  89934, 111439, 104059,\n",
      "          89934, 104059,  85879,   6260],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212,  17212,  32152,\n",
      "          66487,  55500,  66487, 105182,  14258,  51309,  49541,  73450,  90805,\n",
      "          82606,  34326, 113529,  21911, 112512,  21911,  28834,   3339,  93635,\n",
      "          34506,  51309,  85879, 118332],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212,  17212,  17212,\n",
      "          17212,  92109,  51651,  55500, 115582,  55500,  64118,  19609,  28163,\n",
      "         106066,  53949, 111108,  89934,  67839,    317,  67839,  59929,  65526,\n",
      "          69037,  34506,  85879,  85879],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866,  61275,  17212,\n",
      "          17212,  17212,  92109,  17212,  90805,  64118,  20036,  82606,  85879,\n",
      "          82606,  90805,  17212,  82606,  67937,    317,  65496,  87259,  40377,\n",
      "          74667,  85879,  51309,  85879]]) ############################################################ torch.Size([10, 40])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541,   5919,  32152,\n",
      "          64499, 100781,  67190,  44435,  92109,  82606,  64499,  64499,  61275,\n",
      "          82606,  34326,   2157,   2496,  61275,  90925, 112512,  92109, 106066,\n",
      "          55500,  68281,  53949,  64499,  64499],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212,  17212,  40377,\n",
      "          40377,  40377,  24530,  81460,  61275, 112504, 101310,  32152,  85879,\n",
      "          17212,  67839, 113529,  89934,  67839, 114149,  81460,  53949,  67839,\n",
      "          81460,  81460,  95542,  64499,  81460],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275,  60849,  17212,\n",
      "          67839,  89322,  23686,  90805,  27157,  80257,  97256,  44435,  27157,\n",
      "         106066,  27157,  66771, 112512,  21911, 112512,     98,  51214,  13448,\n",
      "         108768,  51309,  85879,  68281,  68281],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119,   7482,  56759,\n",
      "          32152, 108297,  44435,  44435,  44435, 103964,  44435,  61275,  17212,\n",
      "          43258, 106066, 106066,  38028,  89934,  21911,   8505,    317,  99284,\n",
      "          27157,  92109, 103553,  68281,  68281],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212,  17212,  34326,\n",
      "          65793,  17212, 111785,  39356,  55500,  50335,   4218,  39356,  82606,\n",
      "          82606,  82606,  55659,  50272,  67839,  21911,  64499,  66623,  81460,\n",
      "          40377, 111439, 111439,  90635,  92109],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919, 112512,  45315,\n",
      "         101310,  61275,  81460,  81460, 114149,  78406,  92109,  46196,  34326,\n",
      "          82606, 113412,  89934,  63545, 112512,  24324,  24324,   3339, 103964,\n",
      "          51309, 108768, 103553,  64499,  64499],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949, 110710,  99284,\n",
      "          22812,  44435,  67190,  56759,  91784,  63357,  39356, 112512,  82606,\n",
      "          89568,  89568,  89934,  89934, 112512,  67839,  89934, 111439, 104059,\n",
      "          89934, 104059,  85879,   6260, 112512],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212,  17212,  32152,\n",
      "          66487,  55500,  66487, 105182,  14258,  51309,  49541,  73450,  90805,\n",
      "          82606,  34326, 113529,  21911, 112512,  21911,  28834,   3339,  93635,\n",
      "          34506,  51309,  85879, 118332, 109599],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212,  17212,  17212,\n",
      "          17212,  92109,  51651,  55500, 115582,  55500,  64118,  19609,  28163,\n",
      "         106066,  53949, 111108,  89934,  67839,    317,  67839,  59929,  65526,\n",
      "          69037,  34506,  85879,  85879,  89934],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866,  61275,  17212,\n",
      "          17212,  17212,  92109,  17212,  90805,  64118,  20036,  82606,  85879,\n",
      "          82606,  90805,  17212,  82606,  67937,    317,  65496,  87259,  40377,\n",
      "          74667,  85879,  51309,  85879,  82606]]) ############################################################ torch.Size([10, 41])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541,   5919,  32152,\n",
      "          64499, 100781,  67190,  44435,  92109,  82606,  64499,  64499,  61275,\n",
      "          82606,  34326,   2157,   2496,  61275,  90925, 112512,  92109, 106066,\n",
      "          55500,  68281,  53949,  64499,  64499,  68281],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212,  17212,  40377,\n",
      "          40377,  40377,  24530,  81460,  61275, 112504, 101310,  32152,  85879,\n",
      "          17212,  67839, 113529,  89934,  67839, 114149,  81460,  53949,  67839,\n",
      "          81460,  81460,  95542,  64499,  81460,  20225],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275,  60849,  17212,\n",
      "          67839,  89322,  23686,  90805,  27157,  80257,  97256,  44435,  27157,\n",
      "         106066,  27157,  66771, 112512,  21911, 112512,     98,  51214,  13448,\n",
      "         108768,  51309,  85879,  68281,  68281,  74667],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119,   7482,  56759,\n",
      "          32152, 108297,  44435,  44435,  44435, 103964,  44435,  61275,  17212,\n",
      "          43258, 106066, 106066,  38028,  89934,  21911,   8505,    317,  99284,\n",
      "          27157,  92109, 103553,  68281,  68281,  68281],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212,  17212,  34326,\n",
      "          65793,  17212, 111785,  39356,  55500,  50335,   4218,  39356,  82606,\n",
      "          82606,  82606,  55659,  50272,  67839,  21911,  64499,  66623,  81460,\n",
      "          40377, 111439, 111439,  90635,  92109,  82606],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919, 112512,  45315,\n",
      "         101310,  61275,  81460,  81460, 114149,  78406,  92109,  46196,  34326,\n",
      "          82606, 113412,  89934,  63545, 112512,  24324,  24324,   3339, 103964,\n",
      "          51309, 108768, 103553,  64499,  64499,  64499],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949, 110710,  99284,\n",
      "          22812,  44435,  67190,  56759,  91784,  63357,  39356, 112512,  82606,\n",
      "          89568,  89568,  89934,  89934, 112512,  67839,  89934, 111439, 104059,\n",
      "          89934, 104059,  85879,   6260, 112512,  85879],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212,  17212,  32152,\n",
      "          66487,  55500,  66487, 105182,  14258,  51309,  49541,  73450,  90805,\n",
      "          82606,  34326, 113529,  21911, 112512,  21911,  28834,   3339,  93635,\n",
      "          34506,  51309,  85879, 118332, 109599,   7408],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212,  17212,  17212,\n",
      "          17212,  92109,  51651,  55500, 115582,  55500,  64118,  19609,  28163,\n",
      "         106066,  53949, 111108,  89934,  67839,    317,  67839,  59929,  65526,\n",
      "          69037,  34506,  85879,  85879,  89934,  59336],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866,  61275,  17212,\n",
      "          17212,  17212,  92109,  17212,  90805,  64118,  20036,  82606,  85879,\n",
      "          82606,  90805,  17212,  82606,  67937,    317,  65496,  87259,  40377,\n",
      "          74667,  85879,  51309,  85879,  82606,  82606]]) ############################################################ torch.Size([10, 42])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541,   5919,  32152,\n",
      "          64499, 100781,  67190,  44435,  92109,  82606,  64499,  64499,  61275,\n",
      "          82606,  34326,   2157,   2496,  61275,  90925, 112512,  92109, 106066,\n",
      "          55500,  68281,  53949,  64499,  64499,  68281,  63487],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212,  17212,  40377,\n",
      "          40377,  40377,  24530,  81460,  61275, 112504, 101310,  32152,  85879,\n",
      "          17212,  67839, 113529,  89934,  67839, 114149,  81460,  53949,  67839,\n",
      "          81460,  81460,  95542,  64499,  81460,  20225,  44435],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275,  60849,  17212,\n",
      "          67839,  89322,  23686,  90805,  27157,  80257,  97256,  44435,  27157,\n",
      "         106066,  27157,  66771, 112512,  21911, 112512,     98,  51214,  13448,\n",
      "         108768,  51309,  85879,  68281,  68281,  74667,  68281],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119,   7482,  56759,\n",
      "          32152, 108297,  44435,  44435,  44435, 103964,  44435,  61275,  17212,\n",
      "          43258, 106066, 106066,  38028,  89934,  21911,   8505,    317,  99284,\n",
      "          27157,  92109, 103553,  68281,  68281,  68281,  92109],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212,  17212,  34326,\n",
      "          65793,  17212, 111785,  39356,  55500,  50335,   4218,  39356,  82606,\n",
      "          82606,  82606,  55659,  50272,  67839,  21911,  64499,  66623,  81460,\n",
      "          40377, 111439, 111439,  90635,  92109,  82606,  59336],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919, 112512,  45315,\n",
      "         101310,  61275,  81460,  81460, 114149,  78406,  92109,  46196,  34326,\n",
      "          82606, 113412,  89934,  63545, 112512,  24324,  24324,   3339, 103964,\n",
      "          51309, 108768, 103553,  64499,  64499,  64499,  85879],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949, 110710,  99284,\n",
      "          22812,  44435,  67190,  56759,  91784,  63357,  39356, 112512,  82606,\n",
      "          89568,  89568,  89934,  89934, 112512,  67839,  89934, 111439, 104059,\n",
      "          89934, 104059,  85879,   6260, 112512,  85879,  66771],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212,  17212,  32152,\n",
      "          66487,  55500,  66487, 105182,  14258,  51309,  49541,  73450,  90805,\n",
      "          82606,  34326, 113529,  21911, 112512,  21911,  28834,   3339,  93635,\n",
      "          34506,  51309,  85879, 118332, 109599,   7408, 104659],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212,  17212,  17212,\n",
      "          17212,  92109,  51651,  55500, 115582,  55500,  64118,  19609,  28163,\n",
      "         106066,  53949, 111108,  89934,  67839,    317,  67839,  59929,  65526,\n",
      "          69037,  34506,  85879,  85879,  89934,  59336,  82606],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866,  61275,  17212,\n",
      "          17212,  17212,  92109,  17212,  90805,  64118,  20036,  82606,  85879,\n",
      "          82606,  90805,  17212,  82606,  67937,    317,  65496,  87259,  40377,\n",
      "          74667,  85879,  51309,  85879,  82606,  82606,  67839]]) ############################################################ torch.Size([10, 43])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541,   5919,  32152,\n",
      "          64499, 100781,  67190,  44435,  92109,  82606,  64499,  64499,  61275,\n",
      "          82606,  34326,   2157,   2496,  61275,  90925, 112512,  92109, 106066,\n",
      "          55500,  68281,  53949,  64499,  64499,  68281,  63487,  85879],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212,  17212,  40377,\n",
      "          40377,  40377,  24530,  81460,  61275, 112504, 101310,  32152,  85879,\n",
      "          17212,  67839, 113529,  89934,  67839, 114149,  81460,  53949,  67839,\n",
      "          81460,  81460,  95542,  64499,  81460,  20225,  44435,  40377],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275,  60849,  17212,\n",
      "          67839,  89322,  23686,  90805,  27157,  80257,  97256,  44435,  27157,\n",
      "         106066,  27157,  66771, 112512,  21911, 112512,     98,  51214,  13448,\n",
      "         108768,  51309,  85879,  68281,  68281,  74667,  68281,  68281],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119,   7482,  56759,\n",
      "          32152, 108297,  44435,  44435,  44435, 103964,  44435,  61275,  17212,\n",
      "          43258, 106066, 106066,  38028,  89934,  21911,   8505,    317,  99284,\n",
      "          27157,  92109, 103553,  68281,  68281,  68281,  92109, 118332],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212,  17212,  34326,\n",
      "          65793,  17212, 111785,  39356,  55500,  50335,   4218,  39356,  82606,\n",
      "          82606,  82606,  55659,  50272,  67839,  21911,  64499,  66623,  81460,\n",
      "          40377, 111439, 111439,  90635,  92109,  82606,  59336,  62609],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919, 112512,  45315,\n",
      "         101310,  61275,  81460,  81460, 114149,  78406,  92109,  46196,  34326,\n",
      "          82606, 113412,  89934,  63545, 112512,  24324,  24324,   3339, 103964,\n",
      "          51309, 108768, 103553,  64499,  64499,  64499,  85879,  68281],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949, 110710,  99284,\n",
      "          22812,  44435,  67190,  56759,  91784,  63357,  39356, 112512,  82606,\n",
      "          89568,  89568,  89934,  89934, 112512,  67839,  89934, 111439, 104059,\n",
      "          89934, 104059,  85879,   6260, 112512,  85879,  66771,  20674],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212,  17212,  32152,\n",
      "          66487,  55500,  66487, 105182,  14258,  51309,  49541,  73450,  90805,\n",
      "          82606,  34326, 113529,  21911, 112512,  21911,  28834,   3339,  93635,\n",
      "          34506,  51309,  85879, 118332, 109599,   7408, 104659,  82606],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212,  17212,  17212,\n",
      "          17212,  92109,  51651,  55500, 115582,  55500,  64118,  19609,  28163,\n",
      "         106066,  53949, 111108,  89934,  67839,    317,  67839,  59929,  65526,\n",
      "          69037,  34506,  85879,  85879,  89934,  59336,  82606,  82606],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866,  61275,  17212,\n",
      "          17212,  17212,  92109,  17212,  90805,  64118,  20036,  82606,  85879,\n",
      "          82606,  90805,  17212,  82606,  67937,    317,  65496,  87259,  40377,\n",
      "          74667,  85879,  51309,  85879,  82606,  82606,  67839,  66771]]) ############################################################ torch.Size([10, 44])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541,   5919,  32152,\n",
      "          64499, 100781,  67190,  44435,  92109,  82606,  64499,  64499,  61275,\n",
      "          82606,  34326,   2157,   2496,  61275,  90925, 112512,  92109, 106066,\n",
      "          55500,  68281,  53949,  64499,  64499,  68281,  63487,  85879, 111439],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212,  17212,  40377,\n",
      "          40377,  40377,  24530,  81460,  61275, 112504, 101310,  32152,  85879,\n",
      "          17212,  67839, 113529,  89934,  67839, 114149,  81460,  53949,  67839,\n",
      "          81460,  81460,  95542,  64499,  81460,  20225,  44435,  40377,  30020],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275,  60849,  17212,\n",
      "          67839,  89322,  23686,  90805,  27157,  80257,  97256,  44435,  27157,\n",
      "         106066,  27157,  66771, 112512,  21911, 112512,     98,  51214,  13448,\n",
      "         108768,  51309,  85879,  68281,  68281,  74667,  68281,  68281,  92109],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119,   7482,  56759,\n",
      "          32152, 108297,  44435,  44435,  44435, 103964,  44435,  61275,  17212,\n",
      "          43258, 106066, 106066,  38028,  89934,  21911,   8505,    317,  99284,\n",
      "          27157,  92109, 103553,  68281,  68281,  68281,  92109, 118332,  68281],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212,  17212,  34326,\n",
      "          65793,  17212, 111785,  39356,  55500,  50335,   4218,  39356,  82606,\n",
      "          82606,  82606,  55659,  50272,  67839,  21911,  64499,  66623,  81460,\n",
      "          40377, 111439, 111439,  90635,  92109,  82606,  59336,  62609,   5919],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919, 112512,  45315,\n",
      "         101310,  61275,  81460,  81460, 114149,  78406,  92109,  46196,  34326,\n",
      "          82606, 113412,  89934,  63545, 112512,  24324,  24324,   3339, 103964,\n",
      "          51309, 108768, 103553,  64499,  64499,  64499,  85879,  68281,  92109],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949, 110710,  99284,\n",
      "          22812,  44435,  67190,  56759,  91784,  63357,  39356, 112512,  82606,\n",
      "          89568,  89568,  89934,  89934, 112512,  67839,  89934, 111439, 104059,\n",
      "          89934, 104059,  85879,   6260, 112512,  85879,  66771,  20674,  81460],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212,  17212,  32152,\n",
      "          66487,  55500,  66487, 105182,  14258,  51309,  49541,  73450,  90805,\n",
      "          82606,  34326, 113529,  21911, 112512,  21911,  28834,   3339,  93635,\n",
      "          34506,  51309,  85879, 118332, 109599,   7408, 104659,  82606,  92729],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212,  17212,  17212,\n",
      "          17212,  92109,  51651,  55500, 115582,  55500,  64118,  19609,  28163,\n",
      "         106066,  53949, 111108,  89934,  67839,    317,  67839,  59929,  65526,\n",
      "          69037,  34506,  85879,  85879,  89934,  59336,  82606,  82606,   9188],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866,  61275,  17212,\n",
      "          17212,  17212,  92109,  17212,  90805,  64118,  20036,  82606,  85879,\n",
      "          82606,  90805,  17212,  82606,  67937,    317,  65496,  87259,  40377,\n",
      "          74667,  85879,  51309,  85879,  82606,  82606,  67839,  66771,  68281]]) ############################################################ torch.Size([10, 45])\n",
      "tensor([[   101,  95648,  86171,  45839,  51214,  43770,  68068,    484,  42460,\n",
      "         112512,  13448, 108553, 108553,  81460,  49541,  49541,   5919,  32152,\n",
      "          64499, 100781,  67190,  44435,  92109,  82606,  64499,  64499,  61275,\n",
      "          82606,  34326,   2157,   2496,  61275,  90925, 112512,  92109, 106066,\n",
      "          55500,  68281,  53949,  64499,  64499,  68281,  63487,  85879, 111439,\n",
      "          95459],\n",
      "        [   101,  41026,  29440,  94638,  94926,   5919, 113195,    484,  90221,\n",
      "            484,  51651,  52472, 108553, 108553,   5919,  17212,  17212,  40377,\n",
      "          40377,  40377,  24530,  81460,  61275, 112504, 101310,  32152,  85879,\n",
      "          17212,  67839, 113529,  89934,  67839, 114149,  81460,  53949,  67839,\n",
      "          81460,  81460,  95542,  64499,  81460,  20225,  44435,  40377,  30020,\n",
      "          25791],\n",
      "        [   101,  45135,  31014,  37297,  37297,  82032,  13824,  58732,  24588,\n",
      "          82606,  82606,  18206,  30144, 108553, 108553,  61275,  60849,  17212,\n",
      "          67839,  89322,  23686,  90805,  27157,  80257,  97256,  44435,  27157,\n",
      "         106066,  27157,  66771, 112512,  21911, 112512,     98,  51214,  13448,\n",
      "         108768,  51309,  85879,  68281,  68281,  74667,  68281,  68281,  92109,\n",
      "          99284],\n",
      "        [   101,  95305,  90805,  40710, 112512, 112512,  30792,    237,    484,\n",
      "          34326, 118932,  61275,  49541,  49541,  24979,  76119,   7482,  56759,\n",
      "          32152, 108297,  44435,  44435,  44435, 103964,  44435,  61275,  17212,\n",
      "          43258, 106066, 106066,  38028,  89934,  21911,   8505,    317,  99284,\n",
      "          27157,  92109, 103553,  68281,  68281,  68281,  92109, 118332,  68281,\n",
      "           5919],\n",
      "        [   101,  45135,  67841,  22437,  81516,  13433,  58732,  17212, 111108,\n",
      "          81460, 111108, 108553, 108553,  17212,  17212,  17212,  17212,  34326,\n",
      "          65793,  17212, 111785,  39356,  55500,  50335,   4218,  39356,  82606,\n",
      "          82606,  82606,  55659,  50272,  67839,  21911,  64499,  66623,  81460,\n",
      "          40377, 111439, 111439,  90635,  92109,  82606,  59336,  62609,   5919,\n",
      "           5919],\n",
      "        [   101,  20901,  64118,  10321,  86374,  45839, 112512,  92109,  82606,\n",
      "          82606, 108553,  82606,  25379, 108553, 108553,   5919, 112512,  45315,\n",
      "         101310,  61275,  81460,  81460, 114149,  78406,  92109,  46196,  34326,\n",
      "          82606, 113412,  89934,  63545, 112512,  24324,  24324,   3339, 103964,\n",
      "          51309, 108768, 103553,  64499,  64499,  64499,  85879,  68281,  92109,\n",
      "          68281],\n",
      "        [   101,  95648,  31014,  86171,  43770,  17212,  89257,  58732,    484,\n",
      "            484, 107022,  17212,  24979,  52472,  59336,  53949, 110710,  99284,\n",
      "          22812,  44435,  67190,  56759,  91784,  63357,  39356, 112512,  82606,\n",
      "          89568,  89568,  89934,  89934, 112512,  67839,  89934, 111439, 104059,\n",
      "          89934, 104059,  85879,   6260, 112512,  85879,  66771,  20674,  81460,\n",
      "          92109],\n",
      "        [   101,  57118,  90805,  90805,    984, 108553, 112512,  17212,  17212,\n",
      "          82606,  82606,  82606,  17212,  17212,  82606,  17212,  17212,  32152,\n",
      "          66487,  55500,  66487, 105182,  14258,  51309,  49541,  73450,  90805,\n",
      "          82606,  34326, 113529,  21911, 112512,  21911,  28834,   3339,  93635,\n",
      "          34506,  51309,  85879, 118332, 109599,   7408, 104659,  82606,  92729,\n",
      "           8505],\n",
      "        [   101,  45135,  96073,  94834,  52269,  65902,  33150, 112512,  61275,\n",
      "            484,  43517,  17212,  17212, 108553,   2143,  17212,  17212,  17212,\n",
      "          17212,  92109,  51651,  55500, 115582,  55500,  64118,  19609,  28163,\n",
      "         106066,  53949, 111108,  89934,  67839,    317,  67839,  59929,  65526,\n",
      "          69037,  34506,  85879,  85879,  89934,  59336,  82606,  82606,   9188,\n",
      "          22812],\n",
      "        [   101, 102133,  45135,  37297,  52460,  83848, 103553,  18206,    484,\n",
      "            484,  82606, 108553, 108553,  17212,  24979,  28866,  61275,  17212,\n",
      "          17212,  17212,  92109,  17212,  90805,  64118,  20036,  82606,  85879,\n",
      "          82606,  90805,  17212,  82606,  67937,    317,  65496,  87259,  40377,\n",
      "          74667,  85879,  51309,  85879,  82606,  82606,  67839,  66771,  68281,\n",
      "          92109]]) ############################################################ torch.Size([10, 46])\n"
     ]
    }
   ],
   "source": [
    "x,y=torch.rand((10,3,300,300)),torch.zeros(10,1)\n",
    "y+=101\n",
    "mem=encoder(x)\n",
    "out,attn,temptok=decoder.evaluate(mem,get_logits=True)\n",
    "print()\n",
    "# print(decoder.training)\n",
    "# y_hat,attention=decoder(enc_memory_list=mem,target=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 45, 119547])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 177, 16138, 11135, 21885, 102, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory=model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.tensor([[0.1,0.1,0.8],[0.1,0.8,0.8],[0.1,0.1,0.8],[0.1,0.1,0.8]])==0.8).sum(axis=1)>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m memoryl\u001b[39m=\u001b[39mencoder(torch\u001b[39m.\u001b[39mrand(\u001b[39m6\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m512\u001b[39m,\u001b[39m512\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'encoder' is not defined"
     ]
    }
   ],
   "source": [
    "decoder.tokenizer(tgt,padding=False,add_special_tokens=False,return_tensors='pt')[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\noorr/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30525"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'data/tokenizer/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\noorr/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n",
      "Using cache found in C:\\Users\\noorr/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n"
     ]
    }
   ],
   "source": [
    "tokenizer=torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')\n",
    "model=torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-uncased')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
