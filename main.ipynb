{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os, logging\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position=torch.arange(max_len).unsqueeze(1)\n",
    "        div_term=1/(10000**(torch.arange(0,d_model,2)/d_model))\n",
    "        pe=torch.zeros(max_len,d_model)\n",
    "        pe[:,0::2]=torch.sin(position*div_term)\n",
    "        pe[:,1::2]=torch.cos(position*div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self,patches):\n",
    "        return self.dropout(patches+self.pe[:patches.shape[1]])\n",
    "\n",
    "\n",
    "\n",
    "# class ImgPatcher(nn.Module):\n",
    "#     def __init__(self,n_patches:int,emb_d:int,channels_first:bool,img_shape:tuple):\n",
    "#         super(ImgPatcher,self).__init__()\n",
    "#         self.channels_first=channels_first\n",
    "#         H,W,C=img_shape\n",
    "#         if self.channels_first:\n",
    "#             C,H,W=img_shape\n",
    "#         self.chw=(C,H,W)\n",
    "#         assert H==W, \"Please make sure image has same height and width\"\n",
    "#         assert H%n_patches==0, \"Please make sure n_patches is factor of image dims H and W\"\n",
    "#         self.n_patches=n_patches\n",
    "#         self.p=H//n_patches\n",
    "#         self.embedding=nn.Linear(H*W*C//n_patches**2,emb_d)                            #n_patches^2 patches of size (H/n_patches*W/n_patches)*C will be created from each image\n",
    "#         print(self.embedding.weight.shape)\n",
    "    \n",
    "#     def forward(self,img):\n",
    "#         img=img.float()\n",
    "#         N=img.size(0)\n",
    "#         if self.channels_first:\n",
    "#             img=img.permute(0,2,3,1)                #N,C,H,W -> N,H,W,C\n",
    "#         patches=torch.zeros(N,self.n_patches**2,self.embedding.weight.shape[1])\n",
    "#         for idx in range(N):\n",
    "#             img_=img[idx]\n",
    "#             for i in range(self.n_patches):\n",
    "#                 for j in range(self.n_patches):\n",
    "#                     patch=img_[i*self.p:(i+1)*self.p,j*self.p:(j+1)*self.p]\n",
    "#                     patches[idx,(i*self.n_patches)+j]=patch.flatten()\n",
    "#         return self.posenc(self.embedding(patches))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.io\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def load_tokenizer(tokenizer_model:str,data_dir:str=None):\n",
    "    if not os.path.exists('data/tokenizer'):\n",
    "        tokenizer=torch.hub.load('huggingface/pytorch-transformers', 'tokenizer',tokenizer_model)\n",
    "        logging.info(\"Updating Tokenizer Vocab on Data\")\n",
    "        #insert code to get to process data_dir\n",
    "        #[tokenizer(i) for i in tqdm(data_dir[:,1])]\n",
    "        tokenizer.save_pretrained('data/tokenizer')\n",
    "        return tokenizer\n",
    "    else:\n",
    "        tokenizer=torch.hub.load('huggingface/pytorch-transformers', 'tokenizer','data/tokenizer/')\n",
    "        return tokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Stream_Dataset(Dataset):\n",
    "    def __init__(self,seq_length,data_dir,img_dir,tokenizer_model='medicalai/ClinicalBERT'):\n",
    "        self.data_dir=data_dir\n",
    "        self.data=pd.read_csv(data_dir+'/'+'result0.csv').to_numpy()\n",
    "        self.img_dir=img_dir\n",
    "        self.max_length=seq_length\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    def __getitem__(self,idx):\n",
    "        img_name=self.data[idx,0]\n",
    "        semantic_str=self.data[idx,1]\n",
    "        img=torchvision.io.read_image(self.data_dir+'/'+self.img_dir+'/'+img_name)\n",
    "        return img,semantic_str\n",
    "# dataset=Stream_Dataset(\n",
    "#         seq_length=50,\n",
    "#         data_dir=\"/home/mehedi/Desktop/raghib/FLICKR-30K IMAGE CAPTIONING/flickr30k_images\",\n",
    "#         img_dir='flickr30k_images',\n",
    "#         tokenizer_model='bert-base-uncased'\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from typing import Optional, Any, Union, Callable\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import MultiheadAttention, LayerNorm,Dropout, Linear,TransformerEncoderLayer\n",
    "def _get_activation_fn(activation: str) -> Callable[[Tensor], Tensor]:\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "\n",
    "    raise RuntimeError(\"activation should be relu/gelu, not {}\".format(activation))\n",
    "\n",
    "class CustomTransformerDecoderLayer(nn.Module):\n",
    "    __constants__ = ['batch_first', 'norm_first']\n",
    "\n",
    "    def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
    "                 activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,\n",
    "                 layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False, \n",
    "                 need_attn: bool = True, device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
    "                                            **factory_kwargs)\n",
    "        self.multihead_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
    "                                                 **factory_kwargs)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "\n",
    "        self.norm_first = norm_first\n",
    "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm3 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "        self.dropout3 = Dropout(dropout)\n",
    "        # Legacy string support for activation function.\n",
    "        if isinstance(activation, str):\n",
    "            self.activation = _get_activation_fn(activation)\n",
    "        else:\n",
    "            self.activation = activation\n",
    "        self.need_attn=need_attn\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super().__setstate__(state)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt: Tensor,\n",
    "        memory: Tensor,\n",
    "        tgt_mask: Optional[Tensor] = None,\n",
    "        memory_mask: Optional[Tensor] = None,\n",
    "        tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "        memory_key_padding_mask: Optional[Tensor] = None,\n",
    "        tgt_is_causal: bool = False,\n",
    "        memory_is_causal: bool = False,\n",
    "    ) -> Tensor:\n",
    "        r\"\"\"Pass the inputs (and mask) through the decoder layer.\n",
    "\n",
    "        Args:\n",
    "            tgt: the sequence to the decoder layer (required).\n",
    "            memory: the sequence from the last layer of the encoder (required).\n",
    "            tgt_mask: the mask for the tgt sequence (optional).\n",
    "            memory_mask: the mask for the memory sequence (optional).\n",
    "            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).\n",
    "            memory_key_padding_mask: the mask for the memory keys per batch (optional).\n",
    "            tgt_is_causal: If specified, applies a causal mask as tgt mask.\n",
    "                Mutually exclusive with providing tgt_mask. Default: ``False``.\n",
    "            memory_is_causal: If specified, applies a causal mask as tgt mask.\n",
    "                Mutually exclusive with providing memory_mask. Default: ``False``.\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        # see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf\n",
    "\n",
    "        x = tgt\n",
    "        if self.norm_first:\n",
    "            x = x + self._sa_block(x=self.norm1(x), attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask, is_causal=tgt_is_causal)\n",
    "            x = x + self._mha_block(self.norm2(x), memory, memory_mask, memory_key_padding_mask, memory_is_causal)\n",
    "            x = x + self._ff_block(self.norm3(x))\n",
    "        else:\n",
    "            x_sa = self.norm1(x + self._sa_block(x=x, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask, is_causal=tgt_is_causal))\n",
    "            x,attn=self._mha_block(x, memory, memory_mask, memory_key_padding_mask, memory_is_causal,)\n",
    "            x = self.norm2(x_sa + x)\n",
    "            x = self.norm3(x + self._ff_block(x))\n",
    "\n",
    "        return x,attn\n",
    "\n",
    "\n",
    "    # self-attention block\n",
    "    def _sa_block(self, x: Tensor,\n",
    "                  attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: bool = False) -> Tensor:\n",
    "        x = self.self_attn(x, x, x,\n",
    "                           attn_mask=attn_mask,\n",
    "                           key_padding_mask=key_padding_mask,\n",
    "                           is_causal=is_causal,\n",
    "                           need_weights=False)[0]\n",
    "        return self.dropout1(x)\n",
    "\n",
    "    # multihead attention block\n",
    "    def _mha_block(self, x: Tensor, mem: Tensor,\n",
    "                   attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: bool = False) -> Tensor:\n",
    "        x,attn= self.multihead_attn(x, mem, mem,\n",
    "                                attn_mask=attn_mask,\n",
    "                                key_padding_mask=key_padding_mask,\n",
    "                                is_causal=is_causal,\n",
    "                                average_attn_weights=self.need_attn)\n",
    "\n",
    "        return self.dropout2(x),attn\n",
    "\n",
    "    # feed forward block\n",
    "    def _ff_block(self, x: Tensor) -> Tensor:\n",
    "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        return self.dropout3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple,Callable,List,Union\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_bert_embedder(tokenizer_model:str,tokenizer):\n",
    "    if not os.path.exists('data/model'):\n",
    "        model=torch.hub.load('huggingface/pytorch-transformers', 'model',tokenizer_model)\n",
    "        model=update_bert_model(model,tokenizer)\n",
    "        return model\n",
    "    else:\n",
    "        model=torch.hub.load('huggingface/pytorch-transformers', 'model','data/model/')\n",
    "        model=update_bert_model(model,tokenizer)\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def update_bert_model(model,tokenizer):\n",
    "    logging.info(\"Updating Model based on Tokenizer\")\n",
    "    model_vocab_len=model.embeddings.word_embeddings.weight.shape[0]\n",
    "    if model_vocab_len!=len(tokenizer):\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    model.save_pretrained('data/model')\n",
    "    return model\n",
    "\n",
    "\n",
    "class ImgPatcher(nn.Module):\n",
    "    def __init__(self,n_patches:int,patch_dims:int,img_chw:Tuple[int,int,int]):\n",
    "        super(ImgPatcher,self).__init__()\n",
    "        C,H,W=img_chw\n",
    "        assert torch.math.ceil(torch.math.sqrt(n_patches))-torch.math.sqrt(n_patches)==0, \"Please make sure n_patches is of square dimensions\"\n",
    "        assert H==W, \"Please make sure that the image is a square image\"\n",
    "        self.n_patches=n_patches\n",
    "        self.patch_dims=patch_dims\n",
    "        self.kernel_size=int(H//torch.math.sqrt(n_patches))\n",
    "        self.conv=nn.Conv2d(in_channels=C,out_channels=self.patch_dims,kernel_size=self.kernel_size,stride=self.kernel_size)\n",
    "    def forward(self,x):\n",
    "        x=self.conv(x)\n",
    "        x=x.permute(0,2,3,1)\n",
    "        x_emb=x.reshape(x.size(0),self.n_patches,x.size(-1))\n",
    "        return x_emb\n",
    "\n",
    "\n",
    "class SpatialStream(nn.Module):\n",
    "    def __init__(self, n_patches:int,img_chw:Tuple[int,int,int],dropout:float=0.1,d_model:int=768,nhead:int=8,dim_feedforward:int=2048,activation:str='relu',num_ts_blocks:int=2):\n",
    "        super(SpatialStream,self).__init__()\n",
    "        self.patcher=ImgPatcher(n_patches,d_model,img_chw)\n",
    "        self.pos_enc=PositionalEncoding(d_model,dropout,n_patches)\n",
    "        self.spattransformer_blocks=nn.ModuleList([TransformerEncoderLayer(d_model,nhead,dim_feedforward,batch_first=True,activation=activation) for i in range(num_ts_blocks)])\n",
    "        \n",
    "    def forward(self,x):\n",
    "        embedded=self.patcher(x)\n",
    "        encoded=self.pos_enc(embedded)\n",
    "        enc_memory_list=[]\n",
    "        for encoder_layer in self.spattransformer_blocks:\n",
    "            encoded=encoder_layer(encoded)\n",
    "            enc_memory_list.append(encoded)\n",
    "        return enc_memory_list\n",
    "\n",
    "class SemanticStream(nn.Module):\n",
    "    def __init__(self,max_seql:int,dropout:float=0.1,num_ts_blocks:int=2,d_model:int=512,nhead:int=8,dim_feedforward:int=2048,activation:str='relu',tokenizer_model='medicalai/ClinicalBERT'):\n",
    "        super(SemanticStream,self).__init__()\n",
    "        self.max_seql=max_seql\n",
    "        self.tokenizer=load_tokenizer(tokenizer_model=tokenizer_model)            #add this to dataloader instead of model along with patching\n",
    "        self.bert_embedding = load_bert_embedder(tokenizer_model=tokenizer_model,tokenizer=self.tokenizer)\n",
    "        self.pos_enc=PositionalEncoding(d_model,dropout,max_seql)\n",
    "        self.semtransformer_blocks=nn.ModuleList([CustomTransformerDecoderLayer(d_model,nhead,dim_feedforward,batch_first=True,activation=activation) for _ in range(num_ts_blocks)])\n",
    "        self.out=nn.Linear(d_model,len(self.tokenizer))\n",
    "    def forward(self,target=None,enc_memory_list=None):\n",
    "        if isinstance(target,type(List[str])):\n",
    "            tgt=self.tokenizer(target,max_length=self.max_seql,padding='max_length' if self.training else True,truncation=True,return_tensors='pt',add_special_tokens=self.training)\n",
    "            tgt_padmask=tgt['attention_mask']==0\n",
    "            tgt=tgt['input_ids']\n",
    "        else:\n",
    "            tgt=target\n",
    "            tgt_padmask=(tgt==0)\n",
    "        tgt_mask=generate_square_subsequent_mask(tgt.shape[-1])\n",
    "        embedded=self.bert_embedding(input_ids=tgt,attention_mask=tgt_padmask).last_hidden_state\n",
    "        encoded=self.pos_enc(embedded)\n",
    "        for idx,(decoder_layer,memory) in enumerate(zip(self.semtransformer_blocks,enc_memory_list)):\n",
    "            encoded,attn=decoder_layer(encoded,memory,tgt_mask=tgt_mask,tgt_key_padding_mask=tgt_padmask,tgt_is_causal=((not self.training) and (idx==0)))\n",
    "            tgt_mask=None\n",
    "            tgt_padmask=None\n",
    "        out=self.out(encoded)\n",
    "        return out,attn\n",
    "        \n",
    "    def evaluate(self,enc_memory_list=None,get_logits=False):\n",
    "        # with torch.no_grad():\n",
    "        batch_size=enc_memory_list[0].shape[0]\n",
    "        target=['[CLS]' for _ in range(batch_size)]\n",
    "        temptok=self.tokenizer(target,padding=False,return_tensors='pt',add_special_tokens=False)['input_ids']\n",
    "        logits=[]\n",
    "        eos_token=self.tokenizer.vocab['[SEP]']\n",
    "        while True:\n",
    "            if len(logits)==self.max_seql:\n",
    "                break\n",
    "            out,attn=self.forward(target=temptok,enc_memory_list=enc_memory_list)\n",
    "            logits.append(out[:,-1].unsqueeze(1))\n",
    "            probs=F.log_softmax(out[:,-1],dim=-1)\n",
    "            top_prob=probs.topk(1)[1]\n",
    "            temptok=torch.cat([temptok,top_prob],axis=1)\n",
    "        \n",
    "        if get_logits:\n",
    "            logits=torch.cat(logits,axis=1)\n",
    "            return out,attn,logits\n",
    "        else:\n",
    "            return out,attn\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "                \n",
    "\n",
    "    def save_bert_embedder(self):\n",
    "        self.bert_embedding.save_pretrained('data/model/')        \n",
    "        self.tokenizer.save_pretrained('data/tokenizer/')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DistilBertTokenizerFast' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m decoder\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mtrain()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DistilBertTokenizerFast' object has no attribute 'train'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1]) torch.Size([5, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mehedi/anaconda3/envs/hf_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2382: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 26])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.rand((5,1,100))\n",
    "print(\n",
    "    torch.max(F.log_softmax(a,dim=-1),dim=-1)[1].shape,\n",
    "    F.log_softmax(a,dim=-1).topk(1)[1].shape\n",
    ")\n",
    "(decoder.tokenizer(y,padding=True,max_length=decoder.max_seql,truncation=False,return_tensors='pt').input_ids==0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mehedi/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated tokens from 31783 rows of data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mehedi/anaconda3/envs/hf_env/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from dataset import *\n",
    "dataset=Stream_Dataset(\n",
    "        data_dir=\"/home/mehedi/Desktop/raghib/FLICKR-30K IMAGE CAPTIONING/flickr30k_images/\",\n",
    "        imgsz=300,\n",
    "        img_dir='flickr30k_images',\n",
    "        tokenizer_model='medicalai/ClinicalBERT',\n",
    "        csv_dir='result0.csv'\n",
    "\n",
    ")\n",
    "dataload=DataLoader(dataset=dataset,batch_size=4)\n",
    "x,y=next(iter(dataload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mehedi/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "Using cache found in /home/mehedi/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    }
   ],
   "source": [
    "activation='relu'\n",
    "num_ts_blocks=2\n",
    "encoder=SpatialStream(n_patches=36,img_chw=(3,300,300),activation=activation,num_ts_blocks=num_ts_blocks)\n",
    "decoder=SemanticStream(max_seql=45,d_model=768,num_ts_blocks=num_ts_blocks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mehedi/anaconda3/envs/hf_env/lib/python3.10/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "memory=encoder(x.float())\n",
    "decoder.eval()\n",
    "out,att,logits=decoder.evaluate(enc_memory_list=memory,get_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mehedi/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "119547"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(load_tokenizer(tokenizer_model='medicalai/ClinicalBERT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010996341705322266\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "torch.tril(torch.ones(45,5))\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 3])\n",
      "torch.Size([10, 4])\n",
      "torch.Size([10, 5])\n",
      "torch.Size([10, 6])\n",
      "torch.Size([10, 7])\n",
      "torch.Size([10, 8])\n",
      "torch.Size([10, 9])\n",
      "torch.Size([10, 10])\n",
      "torch.Size([10, 11])\n",
      "torch.Size([10, 12])\n",
      "torch.Size([10, 13])\n",
      "torch.Size([10, 14])\n",
      "torch.Size([10, 15])\n",
      "torch.Size([10, 16])\n",
      "torch.Size([10, 17])\n",
      "torch.Size([10, 18])\n",
      "torch.Size([10, 19])\n",
      "torch.Size([10, 20])\n",
      "torch.Size([10, 21])\n",
      "torch.Size([10, 22])\n",
      "torch.Size([10, 23])\n",
      "torch.Size([10, 24])\n",
      "torch.Size([10, 25])\n",
      "torch.Size([10, 26])\n",
      "torch.Size([10, 27])\n",
      "torch.Size([10, 28])\n",
      "torch.Size([10, 29])\n",
      "torch.Size([10, 30])\n",
      "torch.Size([10, 31])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([10, 33])\n",
      "torch.Size([10, 34])\n",
      "torch.Size([10, 35])\n",
      "torch.Size([10, 36])\n",
      "torch.Size([10, 37])\n",
      "torch.Size([10, 38])\n",
      "torch.Size([10, 39])\n",
      "torch.Size([10, 40])\n",
      "torch.Size([10, 41])\n",
      "torch.Size([10, 42])\n",
      "torch.Size([10, 43])\n",
      "torch.Size([10, 44])\n"
     ]
    }
   ],
   "source": [
    "x,y=torch.rand((10,3,300,300)),torch.zeros(10,1)\n",
    "y+=101\n",
    "mem=encoder(x)\n",
    "out,attn,temptok=decoder.evaluate(mem,get_logits=False)\n",
    "# print(decoder.training)\n",
    "# y_hat,attention=decoder(enc_memory_list=mem,target=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   101,  46584, 119100,  97100,  99040,   3027,  99040,  97635,  97100,\n",
       "          38319,  49423, 110284, 110284,  49423,   5004,  13333,   5004,   5004,\n",
       "          77813,  77813,  20682,  20682,  23783,  13681,  25914,  25914,  25755,\n",
       "          25755,   6857, 118305,  49423,  49423,  55916,  29873,  22443,  48768,\n",
       "          99040,  99040,  99040,  52672,  52672,  52672,  99040,  99040,   9765],\n",
       "        [   101,  46584, 119100,  97100,  99040,  99040,  99040,  99040,  99040,\n",
       "          53284,  49423,  99040,  97323,  97635,   3027,  13333,  81185, 112429,\n",
       "          16866,  62147,  25755,  39541, 110284, 110284,  25914,  41502, 110233,\n",
       "          19793,  99040,  12892,  14672, 102439, 102439, 102439,  49423,  48768,\n",
       "          48768,  48768,  48768,  48768,  48768,  48768,  48768,  48768,  49983],\n",
       "        [   101,  11655,   9765,  97100,  97100,  99040,  99040,  99040,  99040,\n",
       "          99040,  97323,  99040,  97323,  97635,   3027,  97635,   3027,  47926,\n",
       "          47926,  47926,  47926,  99040,  99040,  99040, 112429,  65670,  99040,\n",
       "          99040,  99040,  99040,  99040, 102856, 102856,  55916,  99040,  99040,\n",
       "          99040,  99040,  99040,  11448,  52672,  52672,  99040,  99040,  99040],\n",
       "        [   101,  46584, 119100,  97100,  99040,   3027,  99040,  97635,  97100,\n",
       "          38319,  49423, 110284, 110284,  49423,   5004,  13333,   5004,   5004,\n",
       "          77813,  77813,  77813,  20682,  25914,  25914,  25914,  25914,  25755,\n",
       "          25755,   6857, 118305,  49423,  99040,  29873,  29873,  48768,  48768,\n",
       "          99040,  99040,  99040,  52672,  52672,  99040,  99040,  99040,   9765],\n",
       "        [   101,  46584,  97100,  99040, 101293,  99040,  99040,  99040,   3027,\n",
       "          79414,  99040,   3027,  97635,   3027,  97635,   3027,  47926,  30575,\n",
       "          47926,  97352,   9554,   9554,  23661,  23661,  25914,  22443,  28493,\n",
       "          52672,  99040,  99040,  99040,  22318,  29873,  29873,  99040,  99040,\n",
       "          99040,  99040,  99040,  52672,  52672,  52672,  99040,  99040,  99040],\n",
       "        [   101,  46584,  97100,  99040, 101293,   3027,  99040,  97635,  97100,\n",
       "          99040,  49423,  99040,  99040,  97635,  97635,   3027,  81185,   5004,\n",
       "          47926,   9554,   9554,   9554, 118402,  25914,  22443,  13681,  22443,\n",
       "          22443, 118305,  49423,  49423,  22443,  29873,  29873,  22443,  22443,\n",
       "          99040,  99040,  99040,  52672,  52672,  52672,  52672,  99040,   9765],\n",
       "        [   101,  46584, 119100,  97100,  97100,  99040,   3027,   1813,  38319,\n",
       "          38319, 110284, 110284,  49423, 110284,  49423,  13333,   5004,  13333,\n",
       "           1813,  47926,  20682,  39541,  23783,  25914,  25914,  22443,  25755,\n",
       "          25755,  19909, 118305,  49423, 102439,  29873,  29873,  48768,  48768,\n",
       "          48768,  48768,  48768,  48768,  48768,  48768,  48768,  48768,  48768],\n",
       "        [   101,  46584, 119100,  97100,  99040,  99040,  99040,  99040,  99040,\n",
       "          99040,  99040,  99040,   3027,  28493,   3027,  28493,  13333,  28493,\n",
       "          97352,  28493,  28493,  28493,  95930,  39541,  19793,  19793,  19793,\n",
       "          19793,  99040,  99040, 102439, 102439, 102439, 102439,  48768,  48768,\n",
       "          48768,  48768,  48768,  18400,  47214,  48768,  48768,  48768,  47214],\n",
       "        [   101,  46584, 119100,  97100,  99040,  99040,  99040,  99040,  53284,\n",
       "          49423,  99040,  99040,  99040,  99040,   3027, 118400, 118400,   5004,\n",
       "           5004,  47926,  60343,  39541,  23661, 110284,  25914,  37187,  19793,\n",
       "          19793,  99040,  99040,  99040, 102439, 102439,  47214,  49423,  99040,\n",
       "          99040,  99040,  99040,  52672,  52672,  52672,  99040,  99040,  99040],\n",
       "        [   101,  46584, 119100,  97100,  99040,  99040,  99040,  99040,  53284,\n",
       "          99040,  99040,  99040,  99040,   3027, 118400, 118400, 118400,   5004,\n",
       "          47926,  97352,  23661,  81185,  23661,  81185,  25914,  37187,  28493,\n",
       "         118305, 118305, 118305, 118305, 102856,  47214,  47214,  49423,  99040,\n",
       "          99040,  99040,  99040,  99040,  52672,  52672,  99040,  99040,  99040]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temptok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 177, 16138, 11135, 21885, 102, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.tensor([[0.1,0.1,0.8],[0.1,0.8,0.8],[0.1,0.1,0.8],[0.1,0.1,0.8]])==0.8).sum(axis=1)>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m memoryl\u001b[39m=\u001b[39mencoder(torch\u001b[39m.\u001b[39mrand(\u001b[39m6\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m512\u001b[39m,\u001b[39m512\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'encoder' is not defined"
     ]
    }
   ],
   "source": [
    "decoder.tokenizer(tgt,padding=False,add_special_tokens=False,return_tensors='pt')[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\noorr/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30525"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'data/tokenizer/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\noorr/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n",
      "Using cache found in C:\\Users\\noorr/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n"
     ]
    }
   ],
   "source": [
    "tokenizer=torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')\n",
    "model=torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-uncased')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
